## Purpose:
This document is a snapshot of the Daemon AI Assistant codebase and architecture, intended to help both LLMs and human reviewers understand system structure, logic flow, and current areas of work.
├# Project: Daemon — Local AI Assistant

## Overview:
Daemon is a local-first, memory-integrated AI assistant running on an RTX 4090. It uses:
- ChromaDB for persistent memory (episodic, semantic, summary layers)
- SentenceTransformer (onnx_mini_lm_l6_v2) for vector embedding
- Cosine similarity + cross-encoder reranking for gating
- A unified prompt builder with fallback to semantic RAG chunks
- Summarization after every 20 interactions to reduce prompt size
- Logging throughout to support introspection and automated debugging

This system aims to simulate human-like memory processing and internal reasoning, with plans for:
- Dream generation from memory (stochastic, through strict filter, built, not connected)
- Goal tracking and internal directives
- Symbolic/emotional tagging (planned)
- Long-term retrieval via structured metadata

## Prompt Use Notes:
- Logs are consistently formatted to help the LLM reason about flow
- Function names and docstrings are written for clarity and codebase self-description
- Prompts assume modular architecture and memory-aware interaction
- Most components are designed to be independently testable and traceable

## Files Referenced in This Log:
- `memory_interface.py`
- `prompt_builder.py` (merged from v2)
- `gate_system.py`
- `config.py`
- `daemon_app.py`

## Current Focus:
- Summaries are now routed through the prompt pipeline
- All prompt building unified into one builder
- Memory retrieval works; cosine threshold tuning in progress
- Pushing to GitHub post-stability check

###Folder Structure Below:
── all-files.txt
├── chroma
│   └── chroma.sqlite3
├── chroma_db
│   └── chroma.sqlite3
├── config
│   ├── config.py
│   ├── __init__.py
│   ├── prompts
│   │   ├── directives.txt
│   │   ├── __init__.py
│   │   └── system_prompts.txt
│   ├── __pycache__
│   │   ├── config.cpython-311.pyc
│   │   └── __init__.cpython-311.pyc
│   ├── schemas
│   │   ├── __init__.py
│   │   └── memory_schema.json
│   └── settings.py
├── core
│   ├── __init__.py
│   ├── orchestrator.py
│   ├── prompt_buiilder_v2.py
│   ├── 
│   ├── 
│   ├── 
│   ├── __pycache__
│   │   ├── __init__.cpython-311.pyc
│   │   ├── orchestrator.cpython-311.pyc
│   │   ├──
│   │   ├── prompt_builder_v2.cpython-311.pyc
│   │   ├── 
│   │   ├── query_processor.cpython-311.pyc
│   │   └── response_generator.cpython-311.pyc
│   ├── query_processor.py
│   └── response_generator.py
├── daemon_debug.log
├── daemon_memory
│   ├── 4818ba26-020e-42dc-abc5-d31aef761c05
│   │   ├── data_level0.bin
│   │   ├── header.bin
│   │   ├── length.bin
│   │   └── link_lists.bin
│   ├── bd33ed1e-abd0-4c2f-bf6c-5a534e7b102a
│   │   ├── data_level0.bin
│   │   ├── header.bin
│   │   ├── length.bin
│   │   └── link_lists.bin
│   ├── c07155d6-6e8c-421a-8c1f-4338142e1290
│   │   ├── data_level0.bin
│   │   ├── header.bin
│   │   ├── length.bin
│   │   └── link_lists.bin
│   ├── chroma.sqlite3
│   └── dc83a143-6b8e-474c-a54d-f31d99a15bc3
│       ├── data_level0.bin
│       ├── header.bin
│       ├── length.bin
│       └── link_lists.bin
├── data
│   ├── chroma
│   ├── chroma_multi
│   │   └── chroma.sqlite3
│   ├── corpus
│   ├── corpus_v4.json
│   ├── last_query_time.json
│   ├── pipeline
│   │   ├── embed_wiki_chunks_to_parquet.py
│   │   ├── extract_wikipedia_articles.py
│   │   ├── Injection_Protection_Feed_to_Embeder.py
│   │   ├── semantic_chunker.py
│   │   └── unified_pipeline.py
│   ├── scripts
│   │   ├── embed_wiki_chunks_to_parquet.py
│   │   ├── extract_wikipedia_articles.py
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   │   ├── embed_wiki_chunks_to_parquet.cpython-311.pyc
│   │   │   ├── extract_wikipedia_articles.cpython-311.pyc
│   │   │   └── semantic_chunker.cpython-311.pyc
│   │   ├── semantic_chunker.py
│   │   └── unified_pipeline.py
│   └── wiki
│       └── enwiki-latest-pages-articles.xml
├── embed_checkpoint.json
├── embedded_parquet
│   ├── AccessibleComputing_Unknown.parquet
│   ├── AfghanistanHistory_Unknown.parquet
│   └── Anarchism_Unknown.parquet
├── embeddings_mmap.dat
├── gui
│   ├── handlers.py
│   ├── launch.py
│   └── __pycache__
│       ├── handlers.cpython-311.pyc
│       └── launch.cpython-311.pyc
├── hierarchical_memory
├── knowledge
│   ├── __pycache__
│   │   ├── __init__.cpython-311.pyc
│   │   ├── semantic_search.cpython-311.pyc
│   │   ├── topic_manager.cpython-311.pyc
│   │   └── WikiManager.cpython-311.pyc
│   ├── semantic_search.py
│   ├── topic_manager.py
│   └── WikiManager.py
├── main.py
├── memory
│   ├── corpus_manager.py
│   ├── memory_coordinator.py
│   ├── memory_interface.py
│   ├── __pycache__
│   │   ├── corpus_manager.cpython-311.pyc
│   │   ├── __init__.cpython-311.pyc
│   │   ├── memory_coordinator.cpython-311.pyc
│   │   └── memory_interface.cpython-311.pyc
│   └── storage
│       ├── chroma_store.py
│       ├── multi_collection_chroma_store.py
│       ├── __pycache__
│       │   ├── chroma_store.cpython-311.pyc
│       │   ├── __init__.cpython-311.pyc
│       │   └── multi_collection_chroma_store.cpython-311.pyc
│       └── {storage}
│           └── __init__.py
├── metadata.parquet
├── models
│   ├── model_manager.py
│   ├── __pycache__
│   │   ├── __init__.cpython-311.pyc
│   │   ├── model_manager.cpython-311.pyc
│   │   └── tokenizer_manager.cpython-311.pyc
│   └── tokenizer_manager.py
├── personality
│   ├── configs
│   │   ├── creative.json
│   │   ├── default.json
│   │   ├── __init__.py
│   │   └── technical.json
│   ├── __init__.py
│   ├── personality_manager.py
│   └── __pycache__
│       ├── __init__.cpython-311.pyc
│       └── personality_manager.cpython-311.pyc
├── processing
│   ├── gate_system.py
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── gate_system.cpython-311.pyc
│   │   └── __init__.cpython-311.pyc
│   ├── reranking.py
│   └── tag_generator.py
├── project_snapshot.py
├── project_snapshot.txt
├── scripts
│   ├── benchmark.py
│   ├── build_faiss_index.py
│   ├── __init__.py
│   ├── migrate_data.py
│   └── test_component.py
├── semantic_chunks
│   └── semantic_chunks_0000.jsonl
├── test_chroma_db
│   ├── chroma.sqlite3
│   └── d8c9add3-320c-40c9-bb9f-9811c78d8332
│       ├── data_level0.bin
│       ├── header.bin
│       ├── length.bin
│       └── link_lists.bin
├── test_corpus.json
├── tests
│   ├── fixtures
│   │   ├── chroma_test
│   │   │   └── chroma.sqlite3
│   │   └── corpus_test.json
│   ├── __init__.py
│   ├── inspect_chroma.py
│   ├── integration
│   │   ├── __init__.py
│   │   └── test_full_pipeline.py
│   ├── memory_test.py
│   ├── mem_route_test.py
│   ├── __pycache__
│   │   ├── __init__.cpython-311.pyc
│   │   └── test_summaries.cpython-311.pyc
│   ├── test_basic_pipeline.py
│   ├── test_file_processor.py
│   ├── test_gated_prompt.py
│   ├── test_memory_coordinator.py
│   ├── test_memory.py
│   ├── test_orchestrator.py
│   ├── test_response_gen.py
│   └── test_summaries.py
├── utils
│   ├── debug_formatter.py
│   ├── file_processor.py
│   ├── __init__.py
│   ├── logging_utils.py
│   ├── __pycache__
│   │   ├── file_processor.cpython-311.p


## README: 
# Daemon AI Assistant Framework

**An advanced Retrieval-Augmented Generation (RAG) system with hierarchical memory, cosine similarity gating, and dynamic personality management**

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## Overview

**Daemon** is a production-ready conversational AI system that enhances language model responses through intelligent context retrieval, hierarchical memory management, and efficient cosine similarity-based gating. Built with a modular architecture, it integrates multiple knowledge sources while maintaining continuity across conversations and sessions.

---

##  Why Daemon over LangChain or Haystack?

Unlike higher-abstraction frameworks, Daemon offers:
- **Direct access to memory internals** (episodic, semantic, procedural, summary)
- **Custom gating logic** with cosine filtering and optional reranking
- **Persistent, modifiable memory across sessions** — suitable for long-term assistants
- **Minimal dependencies** for faster dev cycles and transparent behavior

Daemon trades some plug-and-play ease for **extensibility, control, and interpretability** — ideal for researchers and tinkerers building truly agentic systems.

---

## Key Features

### Intelligent Memory Management

- **Hierarchical Architecture**:
  - *Episodic*: Timestamped conversation turns
  - *Semantic*: Extracted facts and claims
  - *Procedural*: How-to tasks and step-by-step knowledge
  - *Summary*: Long-term memory compression
  - *Meta*: Memories about memory itself

- **Consolidation** every 20 interactions to prevent overflow
- **Temporal Decay** with relevance-weighted retrieval
- **Persistent Across Sessions** — memory lives forever

---

###  Efficient Cosine Similarity Gating

- Multi-stage pipeline:
  1. FAISS semantic search (top 50)
  2. Cosine filter (threshold = 0.65)
  3. Hierarchical expansion
  4. (Optional) Cross-encoder reranking

- Batch processing with NumPy
- Special handling for meta/system queries
- Graceful fallback when no high-similarity results found

---

###  Advanced Knowledge Integration

- **Wikipedia API** integration with summarization
- **Semantic search** over embedded documents
- **Topic extraction** using SpaCy entity recognition
- **Multi-source fusion** in the prompt builder

---

###  Dynamic Personality System

- Hot-swappable personalities define:
  - Prompt + directive files
  - Memory access patterns
  - Tone, verbosity, and source preferences

Built-ins:
- `Default`: Balanced helper
- `Therapy`: Long memory span, empathetic
- `Snarky`: Minimal memory, witty tone

---

###  Streaming Response Generation

- `async/await` support
- Token-by-token streaming output
- Local or API-based model backends
- Graceful recovery from stream failures

---

###  Production-Ready Tooling

- Full logging (DEBUG, INFO, WARNING)
- Execution time decorators
- Prompt truncation and token budget awareness
- File parsing: TXT, DOCX, CSV, Python
- Time awareness: Session timestamps and response duration

---

##  Dreams and Truth Evaluation (Experimental)

Daemon includes a unique **"dreaming" system**:
- Memories are recombined and reweighted during idle cycles to form *symbolic insight paths* (dreams)
- Each memory is scored with a **truth scalar** based on post-convo evaluation
- Inaccurate memories decay or mutate; consistent ones stabilize
- Enables long-term *adaptive learning without full RL*

This system helps avoid AI "hallucination lock-in" by reevaluating ideas over time.

---

## Interface Preview (Coming Soon)

<!-- Insert a GIF or screenshot here -->
> Screenshot of Gradio interface showing memory selection, personalities, and live chat stream.

---

##  Architecture Diagram
User Interface
```
                       (Gradio Web App)
                             │
                             ▼
┌─────────────────────────────────────────────────────────┐
│                    Core Orchestrator                    │
│  Personality Manager • Response Generator • File Parser │
└─────────────────────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────┐
│                 Prompt Building Pipeline                │
│   Unified Hierarchical • Gated Builder • Topic Manager │
└─────────────────────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────┐
│                    Memory Systems                       │
│  Memory Coordinator • Hierarchical Memory • Corpus Mgr │
└─────────────────────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────┐
│                  Knowledge & Storage                    │
│   Multi-Collection ChromaDB • FAISS • Wikipedia API    │
└─────────────────────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────┐
│                    Gating System                        │
│      Cosine Similarity Filter • Cross-Encoder Rerank   │
└─────────────────────────────────────────────────────────┘
```
##  Configuration

Edit values in `config/config.py`:

```python
GATE_REL_THRESHOLD = 0.65
MAX_FINAL_MEMORIES = 5
DEFAULT_MAX_TOKENS = 2048
CORPUS_FILE = "data/corpus.json"
CHROMA_PATH = "data/chroma_db"
```

## **Project Structure**
```
daemon-ai-assistant/
├── core/               # Core orchestrator and stream gen
├── memory/             # Episodic, semantic, procedural, etc.
├── knowledge/          # Wikipedia + semantic chunks
├── processing/         # Gating and filter systems
├── personality/        # Personalities + directives
├── models/             # LLM manager
├── gui/                # Gradio frontend
├── utils/              # Logging, timing, loaders
├── config/             # Constants and paths
└── tests/              # Unit and integration tests
```

##  Performance Benchmarks

| Task                 | Latency         |
|----------------------|-----------------|
| Memory Retrieval     | <100ms (10k)    |
| Cosine Gating        | ~50ms (50 items)|
| Wikipedia Search     | ~500ms          |
| Response Gen (GPT-4) | ~20–30 tok/s    |
| Memory Usage         | ~2GB + model    |

---

## 🧬 Advanced Usage

### Add New Personality

```python
"custom": {
    "system_prompt_file": "system_prompt_custom.txt",
    "directives_file": "directives_custom.txt",
    "num_memories": 10,
    "include_wiki": True,
    "include_semantic_search": True
}
```

### Query Memory System

```python
memories = await memory_system.hierarchical_memory.retrieve_relevant_memories(
    query="how does cosine similarity work?", max_memories=10
)

results = memory_system.chroma_store.search_conversations("cosine", n_results=5)
```

### Adjust Gating Sensitivity

```python
gate_system = MultiStageGateSystem(model_manager, cosine_threshold=0.70)
```

## FAQ

**Q: Can I run this fully locally?**  
A: Yes — you can use local LLMs (e.g., Mistral, GPT-Neo) and offline embeddings.

**Q: Is there a GUI?**  
A: Yes! Gradio-based web UI included. CLI mode also supported.

**Q: Does it work with open-weight models?**  
A: Yes. Local support works with models like LLaMA, Mixtral, Mistral, etc.

## Getting Started

### Prerequisites

- Python 3.8+
- 16GB+ RAM (recommended)
- 50GB+ disk space
- CUDA GPU (optional for speed)

### Installation (not yet tested outside Fedora 42)

```bash
git clone https://github.com/yourusername/daemon-ai-assistant
cd daemon-ai-assistant
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt
python -m spacy download en_core_web_sm
export OPENAI_API_KEY="your-key-here"  # optional
```

### Quick Start

```bash
# Start web interface
python main.py

# CLI mode
python main.py cli
```

## Contributing

1. Fork this repo  
2. Create a feature branch  
3. Add relevant tests  
4. Open a pull request

## License

MIT License. See LICENSE for details.

## Acknowledgements

- Sentence Transformers
- ChromaDB
- FAISS
- Gradio
- spaCy

### .py files below:
# daemon_7_11_25_refactor/main.py
import asyncio
import sys
import os
from utils.logging_utils import get_logger
from utils.time_manager import TimeManager
from datetime import datetime# Setup logging
logger = get_logger("main")

# Add parent directory to path
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)

# Import our components
from core.response_generator import ResponseGenerator
from core.orchestrator import DaemonOrchestrator
from utils.file_processor import FileProcessor
from memory.memory_coordinator import MemoryCoordinator
from memory.corpus_manager import CorpusManager
from memory.memory_interface import HierarchicalMemorySystem
from memory.storage.chroma_store import MultiCollectionChromaStore
from processing.gate_system import MultiStageGateSystem
from gui.launch import launch_gui
from knowledge.topic_manager import TopicManager
from knowledge.WikiManager import WikiManager
from models.tokenizer_manager import TokenizerManager

# Import model manager
try:
    from models.model_manager import ModelManager
except ImportError:
    logger.warning("ModelManager not found, using mock")
    # Create a mock ModelManager for testing
    class ModelManager:
        def __init__(self):
            self.model_name = "gpt-4-turbo"

        def load_openai_model(self, name, alias):
            logger.info(f"Mock: Loading {name} as {alias}")

        def switch_model(self, name):
            logger.info(f"Mock: Switching to {name}")
            self.model_name = name

        def get_active_model_name(self):
            return self.model_name

        async def generate_async(self, prompt):
            # Mock response that simulates streaming
            async def mock_stream():
                words = "This is a mock response. The calculation 2+2 equals 4.".split()
                for word in words:
                    yield type('obj', (object,), {
                        'choices': [type('obj', (object,), {
                            'delta': type('obj', (object,), {'content': word + ' '})()
                        })()]
                    })()
            return mock_stream()

        def close(self):
            logger.info("Mock: Closing model manager")

#Instantate Time Manager
time_manager = TimeManager()
topic_manager=TopicManager()
wiki_manager = WikiManager()
tokenizer_manager=TokenizerManager()
try:
    from core.prompt_builder_v2 import UnifiedPromptBuilder
    from processing.gate_system import GatedPromptBuilder
    logger.info("Found prompt builders, will use enhanced mode")
except ImportError as e:
    logger.warning(f"Prompt builders not found: {e}, will use raw mode")
def build_orchestrator():
    """Builds and returns a configured orchestrator"""
    model_manager = ModelManager()
    model_manager.load_openai_model("gpt-4-turbo", "gpt-4-turbo")
    model_manager.switch_model("gpt-4-turbo")
    logger.info(f"[ModelManager] Active model set to: {model_manager.get_active_model_name()}")
    response_generator = ResponseGenerator(model_manager)
    file_processor = FileProcessor()

    # Build the prompt builder directly (UnifiedPromptBuilder is now standalone)
    chroma_store = MultiCollectionChromaStore(persist_directory="daemon_memory")
    # ✅ Explicitly grab the semantic collection
    semantic_collection = chroma_store.collections.get("semantic")

    # ✅ Initialize hierarchical memory with correct collection
    hierarchical_memory = HierarchicalMemorySystem(
        model_manager=model_manager,
        chroma_store=chroma_store,
    )

    # ✅ MemoryCoordinator with working memory system
    memory_system = MemoryCoordinator(
        corpus_manager=CorpusManager(),
        chroma_store=chroma_store,
        hierarchical_memory=hierarchical_memory,
        gate_system=MultiStageGateSystem(model_manager)
    )

    logger.info(f"[Memory Boot] Chroma collection set: {semantic_collection is not None}")
    logger.info(f"[Memory Boot] Loaded hierarchical memory count: {len(hierarchical_memory.memories)}")

    prompt_builder = UnifiedPromptBuilder(
        model_manager=model_manager,
        memory_coordinator=memory_system,
        tokenizer_manager=TokenizerManager(),
        wiki_manager=wiki_manager,
        topic_manager=TopicManager(),
        gate_system=MultiStageGateSystem(model_manager)
    )

    from personality.personality_manager import PersonalityManager
    return DaemonOrchestrator(
        model_manager=model_manager,
        response_generator=response_generator,
        file_processor=file_processor,
        prompt_builder=prompt_builder,
        personality_manager=PersonalityManager(),
        memory_system=memory_system
    )


async def test_orchestrator():
    orchestrator = build_orchestrator()
    print("\n" + "="*50)
    print("Testing Orchestrator")
    print("="*50)

    # Test 1: Simple query
    response, debug_info = await orchestrator.process_user_query("What's 2+2? Give a brief answer.")
    print(f"\nResponse: {response}")
    for k, v in debug_info.items():
        if k != 'prompt': print(f"  {k}: {v}")



# Optional test helpers
async def test_prompt_with_summaries():
    orchestrator = build_orchestrator()
    corpus_manager = orchestrator.memory_system.corpus_manager

    if len(corpus_manager.get_summaries()) < 2:
        print("Creating summaries...")
        corpus_manager.create_summary_now(5)
        corpus_manager.create_summary_now(10)

    prompt = await orchestrator.prompt_builder.build_prompt(
        user_input="Summarize our previous conversations.",
        include_dreams=False,
        include_wiki=False,
        include_semantic=False,
        include_summaries=True
    )

    print("\n" + "=" * 60)
    print("PROMPT WITH SUMMARIES")
    print("=" * 60)
    if "[CONVERSATION SUMMARIES]" in prompt:
        start = prompt.find("[CONVERSATION SUMMARIES]")
        end = prompt.find("\n[", start + 1)
        print(prompt[start:end if end != -1 else None])
    else:
        print("❌ No summary section found in prompt!")
    print("\nTotal prompt length:", len(prompt))


async def inspect_summaries():
    orchestrator = build_orchestrator()
    memory_system = orchestrator.memory_system
    corpus_manager = orchestrator.memory_system.corpus_manager
    summaries = memory_system.corpus_manager.get_summaries(limit=10)

    print("\n" + "=" * 60)
    print("SUMMARY INSPECTION")
    print("=" * 60)

    if not summaries:
        print("No summaries found.")
        return

    for i, s in enumerate(summaries):
        timestamp = s.get("timestamp", "Unknown")
        if isinstance(timestamp, datetime):
            timestamp = timestamp.strftime("%Y-%m-%d %H:%M:%S")
        print(f"Summary {i+1}: [{timestamp}] {s.get('tags', [])}")
        print(f"  {s.get('response', '')[:200]}...\n{'-' * 40}")


if __name__ == "__main__":
    try:
        mode = sys.argv[1] if len(sys.argv) > 1 else "gui"

        if mode == "cli":
            asyncio.run(test_orchestrator())
        elif mode == "test-summaries":
            from tests.test_summaries import test_summary_integration
            asyncio.run(test_summary_integration())
        elif mode == "inspect-summaries":
            asyncio.run(inspect_summaries())
        elif mode == "test-prompt-summaries":
            asyncio.run(test_prompt_with_summaries())
        else:
            orchestrator = build_orchestrator()
            launch_gui(orchestrator)

    except KeyboardInterrupt:
        print("\nInterrupted by user")
    except Exception as e:
        logger.error(f"Startup failed: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nShutting down...")
        if hasattr(ModelManager, 'close'):
            ModelManager().close()

# config.py
import os
from utils.logging_utils import get_logger

logger = get_logger("config")
logger.debug("config.py is alive")

VERSION = os.getenv("DAEMON_VERSION", "v4")

# Base directory for data (production)
DEFAULT_DATA_DIR = os.path.join(os.getcwd(), "data")

# Construct default production paths
_default_corpus_file = os.path.join(
    DEFAULT_DATA_DIR, f"corpus_{VERSION}.json"
)
_default_chroma_dir = os.path.join(
    DEFAULT_DATA_DIR, f"chroma_db_{VERSION}"
)

# Allow overrides via ENV
CORPUS_FILE = os.getenv("CORPUS_FILE", _default_corpus_file)
CHROMA_PATH = os.getenv("CHROMA_PATH", _default_chroma_dir)

logger.debug(f"Using CORPUS_FILE={CORPUS_FILE}")
logger.debug(f"Using CHROMA_PATH={CHROMA_PATH}")

#DREAM_FILE = f"dreams_{VERSION}.json"
#DREAM_LOG = f"dream_log_{VERSION}.jsonl"
IN_HARM_TEST = False 
DEFAULT_MODEL_NAME = "llama" 
DREAM_MODEL_NAME="gpt-neo"
DEFAULT_MAX_TOKENS=2048
DEFAULT_TOP_P=.9
DEFAULT_TOP_K=5
DEFAULT_TEMPERATURE=0.7
LOCAL_MODEL_CONTEXT_LIMIT = 4096  # or whatever your largest local
API_MODEL_CONTEXT_LIMIT = 128000 # GPT-4-Turbo etc.
LOAD_LOCAL_MODEL=True
SEMANTIC_ONLY_MODE = False
DEBUG_MODE = True  # Toggle debug info
CONFIDENCE_THRESHOLD=1.5
GATE_REL_THRESHOLD = 0.35 # temp for debug        # Minimum relevance score to pass LLM gate (0-1)
MAX_FINAL_MEMORIES = 5
RERANK_USE_LLM = True
CROSS_ENCODER_WEIGHT = 0.7# Maximum memories to return after all filtering
MEM_NO = 5                      # Number of memories to expand hierarchically
MEM_IMPORTANCE_SCORE = 0.6      # Minimum importance for parent/child expansion
MAX_WORKING_MEMORY=10
CHILD_MEM_LIMIT = 3  # Max children to include per memory
##Memory module promts
COSINE_SIMILARITY_THRESHOLD=0.25 #try values between .3 and .7, experiement
DEFAULT_SUMMARY_PROMPT_HEADER = "Summary of last 20 exchanges:\n"
DEFAULT_TAGGING_PROMPT = """You are Daemon, my assistant. Extract 5 concise tags or keywords from the following input. Return them as a list, comma-separated.

Input: "{text}"

Tags:"""
OpenAPIKey = os.getenv("OPENAI_API_KEY", "")



# === Imports ===
import torch
from datetime import datetime


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

SYSTEM_PROMPT = "You are Daemon, a brutally honest AI assistant who tells the truth even when uncomfortable, admits ignorance freely, and uses dark humor about current events. You're willing to call out BS while still being genuinely helpful."


DEFAULT_CORE_DIRECTIVE = {
    "query": "[CORE DIRECTIVE]",
    "response": (
        "You are an AI assistant. You are helpful, reliable, and memory-persistent.\n"
        "You should respect user intent and be aligned with their goals."
    ),
    "timestamp": datetime.now(),
    "tags": ["@seed", "core", "directive", "safety"]
}



def set_padding_token_if_none(tokenizer):
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer
# daemon_7_11_25_refactor/core/__init__.py
"""
Core module for the Daemon AI Assistant
"""

from .orchestrator import DaemonOrchestrator
from .response_generator import ResponseGenerator

__all__ = [
    'DaemonOrchestrator',
    'ResponseGenerator',
]

# Optional imports that may not be ready yet
try:
    from .prompt_builder import UnifiedHierarchicalPromptBuilder
    __all__.append('UnifiedHierarchicalPromptBuilder')
except ImportError:
    pass

try:
    from .query_processor import QueryProcessor
    __all__.append('QueryProcessor')
except ImportError:
    pass
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
from utils.logging_utils import get_logger
from memory.corpus_manager import CorpusManager
from memory.memory_coordinator import MemoryCoordinator
from memory.storage.chroma_store import MultiCollectionChromaStore
from memory.memory_interface import HierarchicalMemorySystem
from personality.personality_manager import PersonalityManager
from knowledge.topic_manager import TopicManager
from knowledge.WikiManager import WikiManager
from processing.gate_system import MultiStageGateSystem
from models.tokenizer_manager import TokenizerManager
from core.prompt_builder_v2 import UnifiedPromptBuilder

logger = get_logger("orchestrator")


class DaemonOrchestrator:
    """Central coordinator for the AI assistant system"""

    def __init__(
        self,
        model_manager,
        response_generator,
        file_processor,
        prompt_builder,
        personality_manager: PersonalityManager = None
    ):
        self.model_manager = model_manager
        self.response_generator = response_generator
        self.file_processor = file_processor
        self.personality_manager = personality_manager or PersonalityManager()
        self.prompt_builder = prompt_builder

        # Initialize components
        self.topic_manager = TopicManager()
        self.wiki_manager = WikiManager()
        self.tokenizer_manager = TokenizerManager(model_manager)

        # Initialize memory system
        self.corpus_manager = CorpusManager()
        self.chroma_store = MultiCollectionChromaStore()
        self.hierarchical_memory = HierarchicalMemorySystem(
            model_manager=model_manager,
            chroma_store=self.chroma_store
        )

        # Initialize memory coordinator
        self.memory_system = MemoryCoordinator(
            corpus_manager=self.corpus_manager,
            chroma_store=self.chroma_store,
            hierarchical_memory=self.hierarchical_memory,
            gate_system=MultiStageGateSystem(model_manager)
        )



    def get_recent_conversation(self, n: int = 5):
        """Get recent conversation history"""
        return self.memory_system.corpus_manager.get_recent_memories(count=n)

    async def process_user_query(
        self,
        user_input: str,
        files: List[Any] = None,
        use_raw_mode: bool = False
    ) -> Tuple[str, Dict]:
        """
        Main entry point for processing user queries.
        Returns: (response, debug_info)
        """
        debug_info = {
            'start_time': datetime.now(),
            'user_input': user_input[:100],
            'files_count': len(files) if files else 0,
            'mode': 'raw' if use_raw_mode else 'enhanced'
        }

        try:
            # Step 1: Process files if any
            combined_text = user_input
            if files and not use_raw_mode:
                combined_text = await self.file_processor.process_files(user_input, files)
                debug_info['combined_text_length'] = len(combined_text)

            # Step 2: Build prompt
            if use_raw_mode:
                prompt = combined_text
            else:
                # Get current personality configuration
                config = self.personality_manager.get_current_config()

                # Read system prompt from file
                system_prompt = ""
                if config.get("system_prompt_file"):
                    try:
                        with open(config["system_prompt_file"], "r") as f:
                            system_prompt = f.read()
                    except:
                        logger.warning(f"Could not read system prompt file: {config['system_prompt_file']}")

                # Build prompt using simplified builder
                prompt = await self.prompt_builder.build_prompt(
                    user_input=combined_text,
                    include_dreams=True,
                    include_wiki=config.get("include_wiki", True),
                    include_semantic=config.get("include_semantic_search", True),
                    include_summaries=True,
                    system_prompt=system_prompt,
                    directives_file=config.get("directives_file", "structured_directives.txt"),
                    personality_config=config
                )

            # Step 3: Generate response
            model_name = self.model_manager.get_active_model_name()
            if not model_name:
                model_name = "gpt-4-turbo"
                self.model_manager.switch_model(model_name)

            full_response = ""
            chunk_count = 0

            async for chunk in self.response_generator.generate_streaming_response(prompt, model_name):
                full_response += chunk + " "
                chunk_count += 1

            # Step 4: Store the interaction in memory
            if not use_raw_mode:
                await self.memory_system.store_interaction(
                    query=user_input,
                    response=full_response.strip(),
                    tags=["conversation"]
                )

            debug_info['response_length'] = len(full_response)
            debug_info['chunk_count'] = chunk_count
            debug_info['end_time'] = datetime.now()
            debug_info['duration'] = (debug_info['end_time'] - debug_info['start_time']).total_seconds()
            debug_info['prompt_length'] = len(prompt)

            return full_response.strip(), debug_info

        except Exception as e:
            logger.error(f"Error processing query: {e}")
            debug_info['error'] = str(e)
            raise
# core/prompt_builder_v2.py
import asyncio
import time
import os
from typing import Dict, List, Optional, Any
from datetime import datetime
from utils.logging_utils import get_logger, log_and_time

logger = get_logger("prompt_builder_v2")

class UnifiedPromptBuilder:
    """
    Unified prompt builder that combines all functionality:
    - Memory retrieval from multiple sources
    - Cosine similarity gating
    - Hierarchical memory expansion
    - Token budget management import PromptBuilder
    from core.prompt_builder import UnifiedHierarchicalPromptBuilder
    - Final prompt assembly
    """

    def __init__(
        self,
        model_manager,
        memory_coordinator,
        tokenizer_manager,
        wiki_manager,
        topic_manager,
        gate_system=None,
        max_tokens=4096,
        reserved_for_output=512
    ):
        self.model_manager = model_manager
        self.memory_coordinator = memory_coordinator
        self.tokenizer_manager = tokenizer_manager
        self.wiki_manager = wiki_manager
        self.topic_manager = topic_manager
        self.gate_system = gate_system
        self.max_tokens = max_tokens
        self.reserved_for_output = reserved_for_output
        self.token_budget = max_tokens - reserved_for_output

    @log_and_time("Build Prompt")
    async def build_prompt(
        self,
        user_input: str,
        include_dreams: bool = True,
        include_wiki: bool = True,
        include_semantic: bool = True,
        system_prompt: str = "",
        directives_file: str = "structured_directives.txt",
        personality_config: Dict = None,
        **kwargs
    ) -> str:
        """
        Main entry point for building prompts.
        Handles all retrieval, gating, and assembly in one place.
        """
        logger.debug(f"[PROMPT] Building prompt for: {user_input[:100]}...")

        # Step 1: Gather all context sources
        context = await self._gather_context(
            user_input,
            include_dreams=include_dreams,
            include_wiki=include_wiki,
            include_semantic=include_semantic,
            personality_config=personality_config
        )

        # Step 2: Apply gating if available
        if self.gate_system:
            context = await self._apply_gating(user_input, context)

        # Step 3: Manage token budget
        context = self._manage_token_budget(context)

        # Step 4: Assemble final prompt
        prompt = self._assemble_prompt(
            user_input=user_input,
            context=context,
            system_prompt=system_prompt,
            directives_file=directives_file
        )

        logger.debug(f"[PROMPT] Final length: {len(prompt)} chars")
        return prompt

    async def _gather_context(
        self,
        user_input: str,
        include_dreams: bool,
        include_wiki: bool,
        include_semantic: bool,
        personality_config: Dict = None
    ) -> Dict[str, Any]:
        """Gather all context sources in parallel"""
        context = {
            "memories": [],
            "summaries": [],
            "dreams": [],
            "wiki": "",
            "semantic_chunks": [],
            "recent_conversations": [],
            "time_context": self._get_time_context()
        }

        # Configure based on personality
        config = personality_config or {}
        memory_count = config.get("num_memories", 10)

        # Parallel retrieval using asyncio.gather
        tasks = []

        # Recent conversations (always included)
        tasks.append(self._get_recent_conversations(5))

        # Memories
        tasks.append(self.memory_coordinator.get_memories(user_input, limit=memory_count))

        # Summaries
        tasks.append(self._get_summaries(3))

        # Dreams (if enabled)
        if include_dreams:
            tasks.append(self._get_dreams(2))
        else:
            tasks.append(asyncio.create_task(asyncio.coroutine(lambda: [])()))

        # Wiki content (if enabled)
        if include_wiki:
            tasks.append(self._get_wiki_content(user_input))
        else:
            tasks.append(asyncio.create_task(asyncio.coroutine(lambda: "")()))

        # Semantic search (if enabled)
        if include_semantic:
            tasks.append(self._get_semantic_chunks(user_input))
        else:
            tasks.append(asyncio.create_task(asyncio.coroutine(lambda: [])()))

        # Execute all tasks in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Unpack results
        context["recent_conversations"] = results[0] if not isinstance(results[0], Exception) else []
        context["memories"] = results[1] if not isinstance(results[1], Exception) else []
        context["summaries"] = results[2] if not isinstance(results[2], Exception) else []
        context["dreams"] = results[3] if not isinstance(results[3], Exception) else []
        context["wiki"] = results[4] if not isinstance(results[4], Exception) else ""
        context["semantic_chunks"] = results[5] if not isinstance(results[5], Exception) else []

        return context

    async def _apply_gating(self, user_input: str, context: Dict) -> Dict:
        """Apply cosine similarity gating to filter context"""
        logger.debug("[PROMPT] Applying cosine similarity gating")

        # Gate memories
        if context["memories"]:
            context["memories"] = await self.gate_system.filter_memories(
                user_input, context["memories"]
            )

        # Gate wiki content
        if context["wiki"]:
            include_wiki, filtered_wiki = await self.gate_system.filter_wiki_content(
                user_input, context["wiki"]
            )
            context["wiki"] = filtered_wiki if include_wiki else ""

        # Gate semantic chunks
        if context["semantic_chunks"]:
            context["semantic_chunks"] = await self.gate_system.filter_semantic_chunks(
                user_input, context["semantic_chunks"]
            )

        logger.debug(f"[PROMPT] After gating: {len(context['memories'])} memories, "
                    f"{len(context['semantic_chunks'])} chunks")

        return context

    def _manage_token_budget(self, context: Dict) -> Dict:
        """Trim context to fit within token budget"""
        model_name = self.model_manager.get_active_model_name()
        current_tokens = 0

        # Priority order for context (most important first)
        priority_order = [
            ("recent_conversations", 5),
            ("memories", 10),
            ("summaries", 3),
            ("semantic_chunks", 5),
            ("wiki", 1),
            ("dreams", 2)
        ]

        trimmed_context = context.copy()

        for key, max_items in priority_order:
            if key not in context or not context[key]:
                continue

            if isinstance(context[key], list):
                # Handle list items
                items = []
                for item in context[key][:max_items]:
                    item_text = self._extract_text(item)
                    item_tokens = self.get_token_count(item_text, model_name)

                    if current_tokens + item_tokens < self.token_budget:
                        items.append(item)
                        current_tokens += item_tokens
                    else:
                        break

                trimmed_context[key] = items
            else:
                # Handle single text items
                text = str(context[key])
                text_tokens = self.get_token_count(text, model_name)

                if current_tokens + text_tokens < self.token_budget:
                    current_tokens += text_tokens
                else:
                    # Trim to fit
                    remaining_budget = self.token_budget - current_tokens
                    if remaining_budget > 50:  # Only include if meaningful
                        trimmed_context[key] = text[:remaining_budget * 4]  # Rough char estimate
                        current_tokens = self.token_budget
                    else:
                        trimmed_context[key] = ""

        logger.debug(f"[PROMPT] Token budget: {current_tokens}/{self.token_budget}")
        return trimmed_context

    def _assemble_prompt(
        self,
        user_input: str,
        context: Dict,
        system_prompt: str,
        directives_file: str
    ) -> str:
        """Assemble the final prompt from all components"""
        prompt_parts = []

        # System prompt
        if system_prompt:
            prompt_parts.append(f"[SYSTEM]\n{system_prompt}\n")

        # Time context
        if context.get("time_context"):
            prompt_parts.append(f"\n[TIME CONTEXT]\n{context['time_context']}\n")

        # Recent conversations (most important for context)
        if context.get("recent_conversations"):
            prompt_parts.append("\n[RECENT CONVERSATION]")
            for i, conv in enumerate(context["recent_conversations"][-5:]):
                q = conv.get("query", "").strip()
                r = conv.get("response", "").strip()
                if q or r:
                    prompt_parts.append(f"User: {q}\nAssistant: {r}\n")

        # Relevant memories
        if context.get("memories"):
            prompt_parts.append("\n[RELEVANT MEMORIES]")
            for mem in context["memories"]:
                prompt_parts.append(self._format_memory(mem))

        # Summaries
        if context.get("summaries"):
            prompt_parts.append("\n[CONVERSATION SUMMARIES]")
            for summary in context["summaries"]:
                prompt_parts.append(f"{summary}\n")

        # Wiki knowledge
        if context.get("wiki"):
            prompt_parts.append(f"\n[BACKGROUND KNOWLEDGE]\n{context['wiki']}\n")

        # Semantic chunks
        if context.get("semantic_chunks"):
            prompt_parts.append("\n[RELEVANT INFORMATION]")
            for chunk in context["semantic_chunks"]:
                text = chunk.get("text", chunk.get("content", ""))
                prompt_parts.append(f"- {text[:300]}...\n")

        # Dreams (if any)
        if context.get("dreams"):
            prompt_parts.append("\n[DREAMS/REFLECTIONS]")
            for dream in context["dreams"]:
                prompt_parts.append(f"{dream}\n")

        # Load and add directives
        directives = self._load_directives(directives_file)
        if directives:
            prompt_parts.append(f"\n[DIRECTIVES]\n{directives}\n")

        # User input
        prompt_parts.append(f"\n[USER INPUT]\n{user_input}\n")

        return "".join(prompt_parts)

    # === Helper Methods ===

    def get_token_count(self, text: str, model_name: str) -> int:
        """Get token count for text"""
        tokenizer = self.tokenizer_manager.get_tokenizer(model_name)
        if tokenizer is None:
            return len(text) // 4  # Rough estimate
        return len(tokenizer.encode(text, truncation=False))

    def _extract_text(self, item: Any) -> str:
        """Extract text from various item formats"""
        if isinstance(item, str):
            return item
        elif isinstance(item, dict):
            # Try various keys
            for key in ["content", "text", "response", "filtered_content"]:
                if key in item:
                    return str(item[key])
            # Fallback to string representation
            return str(item)
        else:
            return str(item)

    def _format_memory(self, memory: Dict) -> str:
        """Format a memory for inclusion in prompt"""
        if "query" in memory and "response" in memory:
            return f"Q: {memory['query']}\nA: {memory['response']}\n"
        elif "content" in memory:
            return f"{memory['content']}\n"
        else:
            return f"{str(memory)}\n"

    def _get_time_context(self) -> str:
        """Get current time context"""
        now = datetime.now()
        return f"Current time: {now.strftime('%Y-%m-%d %H:%M:%S')}"

    async def _get_recent_conversations(self, count: int) -> List[Dict]:
        """Get recent conversations"""
        return self.memory_coordinator.corpus_manager.get_recent_memories(count)

    async def _get_summaries(self, count: int) -> List[str]:
        """Get conversation summaries"""
        summaries = self.memory_coordinator.get_summaries(limit=count)
        return [s.get('content', '') for s in summaries if s.get('content')]

    async def _get_dreams(self, count: int) -> List[str]:
        """Get dreams/reflections"""
        dreams = self.memory_coordinator.get_dreams(limit=count)
        return [d.get('content', '') for d in dreams if d.get('content')]

    async def _get_wiki_content(self, query: str) -> str:
        """Get wiki content for query"""
        topics = self.topic_manager.update_from_user_input(query)
        wiki_content = []

        for topic in topics[:3]:  # Limit to top 3 topics
            snippet = self.wiki_manager.search_summary(topic)
            if snippet and not snippet.startswith("["):
                wiki_content.append(f"{topic}: {snippet}")

        return "\n".join(wiki_content)

    async def _get_semantic_chunks(self, query: str) -> List[Dict]:
        """Get semantic search results"""
        from knowledge.semantic_search import semantic_search
        return semantic_search(query, top_k=10)

    def _load_directives(self, directives_file: str) -> str:
        """Load directives from file"""
        if os.path.exists(directives_file):
            with open(directives_file, 'r') as f:
                return f.read()
        return ""
# daemon_7_11_25_refactor/core/response_generator.py
from utils.logging_utils import get_logger
import time
from typing import AsyncGenerator
from datetime import datetime
from utils.time_manager import TimeManager
logger = get_logger("response_generator")


class ResponseGenerator:
    """Handles response generation and streaming"""

    def __init__(self, model_manager, time_manager: TimeManager = None):
        self.model_manager = model_manager
        self.time_manager = time_manager or TimeManager()
        self.logger = logger

    async def generate_streaming_response(self,
                                        prompt: str,
                                        model_name: str = None) -> AsyncGenerator[str, None]:
        """
        Generate response with streaming support
        """
        self.logger.debug(f"[GENERATE] Starting async generation with model: {model_name}")
        start_time = time.time()
        self.time_manager.mark_query_time()
        self.logger.debug(f"[TIME] Since last query: {self.time_manager.elapsed_since_last()}")
        self.logger.debug(f"[TIME] Previous response time: {self.time_manager.last_response()}")

        first_token_time = None

        try:
            if model_name:
                self.model_manager.switch_model(model_name)
                logger.info(f"[ModelManager] Active model set to: {self.model_manager.get_model()}")

            # Get the async generator
            response_generator = await self.model_manager.generate_async(prompt)

            # Check if it's an async generator
            if hasattr(response_generator, "__aiter__"):
                buffer = ""
                async for chunk in response_generator:
                    try:
                        # Extract content from ChatCompletionChunk
                        if hasattr(chunk, 'choices') and len(chunk.choices) > 0:
                            delta = chunk.choices[0].delta
                            if hasattr(delta, 'content') and delta.content:
                                delta_content = delta.content
                            else:
                                delta_content = ""
                        else:
                            delta_content = ""

                        if delta_content:
                            now = time.time()
                            if first_token_time is None:
                                first_token_time = now
                                self.logger.debug(f"[STREAMING] First token arrived after {now - start_time:.2f} seconds")

                            buffer += delta_content

                            # For the mock, just yield word by word
                            if " " in buffer:
                                words = buffer.split(" ")
                                for word in words[:-1]:
                                    if word:
                                        yield word
                                buffer = words[-1] if words[-1] else ""

                    except Exception as e:
                        self.logger.error(f"[STREAMING] Error processing chunk: {e}")
                        continue

                # Yield remaining buffer
                if buffer.strip():
                    yield buffer.strip()
                    end_time = time.time()
                    duration = self.time_manager.measure_response(
                        datetime.fromtimestamp(start_time),
                        datetime.fromtimestamp(end_time)
                    )
                    self.logger.info(f"[TIMING] Full response duration: {duration}")

            else:
                # Handle non-streaming response (synchronous fallback)
                self.logger.debug("[GENERATE] Non-streaming response, handling in fallback mode.")

                # If generate_async returned a regular response object
                if hasattr(response_generator, "choices") and len(response_generator.choices) > 0:
                    if hasattr(response_generator.choices[0], "message"):
                        content = response_generator.choices[0].message.content
                    else:
                        content = str(response_generator)
                else:
                    content = str(response_generator)

                # Simulate streaming by yielding words
                words = content.split()
                for word in words:
                    yield word

        except Exception as e:
            self.logger.error(f"[GENERATE] Error: {type(e).__name__}: {str(e)}")
            yield f"[Streaming Error] {e}"
import pandas as pd
import logging
from utils.logging_utils import log_and_time
import json
from config.config import SYSTEM_PROMPT as system_prompt

logger = logging.getLogger("gradio_gui")


def smart_join(prev: str, new: str) -> str:
    """
    Inserts a space between tokens unless the new chunk begins with punctuation or whitespace.
    Prevents jammed-together words while respecting formatting.
    """
    if not prev:
        return new
    if prev.endswith((' ', '\n')) or new.startswith((' ', '\n', '.', ',', '?', '!', "'", '"', ")", "’", "”")):
        return prev + new
    else:
        return prev + ' ' + new


@log_and_time("Handle Submit")
async def handle_submit(user_text, files, history, use_raw_gpt, orchestrator, force_summarize=False, include_summaries=True):

    # Check if we should force a summary
    if force_summarize:
        summary = orchestrator.memory_system.corpus_manager.create_summary_now()
        if summary:
            logger.info("Forced summary creation completed")
    logger.debug(f"[Handle Submit] Received user_text: {user_text}")

    if not user_text.strip():
        yield {"role": "assistant", "content": "⚠️ Empty input received."}
        return

    # Process files into the 'file_data' list
    file_data = []
    if files:
        for file in files:
            try:
                if file.name.endswith(".txt"):
                    with open(file.name, 'r', encoding='utf-8') as f:
                        file_data.append(f.read())
                elif file.name.endswith(".csv"):
                    df = pd.read_csv(file.name)
                    file_data.append(df.to_string())
                elif file.name.endswith(".py"):
                    with open(file.name, 'r', encoding='utf-8') as f:
                        file_data.append(f.read())
                else:
                    file_data.append(f"[Unsupported file type: {file.name}]")
            except Exception as e:
                file_data.append(f"[Error reading {file.name}: {str(e)}]")

    # Step 1: Merge file content into user_input
    user_input = user_text + "\n\n" + "\n\n".join(file_data)

    # Step 2: Build full hierarchical prompt
    full_prompt = await orchestrator.prompt_builder.build_prompt(
        user_input=user_input,
        include_dreams=True,
        include_wiki=True,
        include_semantic=True,
    )

    logger.debug(f"[Handle Submit] Final prompt being passed to model:\n{full_prompt}")

    final_output = ""
    try:
        # Step 3: Stream response from full prompt
        logger.debug(f"[🔍 FINAL MESSAGE PAYLOAD TO OPENAI]:\n{json.dumps([{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': full_prompt}], indent=2)}")

        async for chunk in orchestrator.response_generator.generate_streaming_response(
            prompt=full_prompt,
            model_name=orchestrator.model_manager.get_active_model_name()
        ):
            final_output = smart_join(final_output, chunk)
            yield {"role": "assistant", "content": final_output}

        if not final_output:
            yield {"role": "assistant", "content": "⚠️ No response was generated."}

    except Exception as e:
        logger.error(f"[HANDLE_SUBMIT] Streaming error: {e}")
        yield {"role": "assistant", "content": f"⚠️ Streaming error: {str(e)}"}

    finally:
        # Step 4: Store the completed interaction in memory.
        # This block will run even if the streaming fails.
        if final_output:  # Only store if any response was generated
            try:
                logger.info("[HANDLE_SUBMIT] Storing interaction in memory...")
                # NOTE: Using the 'memory_system' which is the MemoryCoordinator
                await orchestrator.memory_system.store_interaction(
                    query=user_input,
                    response=final_output
                )
                logger.info("[HANDLE_SUBMIT] Interaction successfully stored.")
            except Exception as e:
                logger.error(f"[HANDLE_SUBMIT] Failed to store interaction: {e}")

import gradio as gr
from gui.handlers import handle_submit

def launch_gui(orchestrator):
    personality_manager = orchestrator.personality_manager

    def get_summary_status():
        summaries = orchestrator.memory_system.corpus_manager.get_summaries()
        total_entries = len(orchestrator.memory_system.corpus_manager.corpus)
        non_summary_entries = len([
            e for e in orchestrator.memory_system.corpus_manager.corpus
            if "@summary" not in e.get("tags", [])
        ])
        return {
            "total_summaries": len(summaries),
            "total_entries": total_entries,
            "non_summary_entries": non_summary_entries,
            "next_summary_in": 20 - (non_summary_entries % 20)
        }

    async def submit_chat(user_text, chat_history, files, use_raw_gpt, personality):
        personality_manager.switch_personality(personality)
        async for chunk in handle_submit(
            user_text=user_text,
            files=files,
            history=chat_history,
            use_raw_gpt=use_raw_gpt,
            orchestrator=orchestrator
        ):
            # If chunk is a dict, extract 'content'
            if isinstance(chunk, dict) and "content" in chunk:
                assistant_reply = chunk["content"]
            else:
                assistant_reply = str(chunk)
            yield chat_history + [[user_text, assistant_reply]]


    with gr.Blocks(theme="soft") as demo:
        gr.Markdown("## 🤖 Daemon Chat Interface")
        with gr.Row():
            summary_json = gr.JSON(value=get_summary_status(), label="📊 Summary Status")
            refresh_button = gr.Button("🔄 Refresh Summary Status")
        refresh_button.click(fn=get_summary_status, outputs=summary_json)

        chatbot = gr.Chatbot(label="Daemon")
        user_input = gr.Textbox(lines=2, placeholder="Ask Daemon something...", label="Your Message")
        submit_button = gr.Button("Submit")

        with gr.Row():
            files = gr.File(file_types=[".txt", ".docx", ".csv", ".py"], file_count="multiple", label="Files")
            use_raw = gr.Checkbox(label="Bypass Memory (Raw GPT)", value=False)
            personality = gr.Dropdown(
                label="Personality",
                choices=list(personality_manager.personalities.keys()),
                value=personality_manager.current_personality
            )

        chat_state = gr.State([])

        submit_button.click(
            submit_chat,
            inputs=[user_input, chat_state, files, use_raw, personality],
            outputs=[chatbot],
        )

    demo.launch(
        server_name="0.0.0.0",
        max_file_size="100mb",
        max_threads=40,
        quiet=False,
        share=True
    )
# daemon_7_11_25_refactor/memory/corpus_manager.py
import json
import os
from datetime import datetime
from typing import List, Dict
from utils.logging_utils import get_logger, log_and_time

logger = get_logger("corpus_manager")

class CorpusManager:
    """Manages the conversation corpus (short-term memory)"""

    def __init__(self, corpus_file: str = None):
        from config.config import CORPUS_FILE
        self.corpus_file = corpus_file or CORPUS_FILE
        self.corpus = self._load_corpus()

    @log_and_time("Load Corpus")
    def _load_corpus(self) -> List[Dict]:
        """Load corpus from disk"""
        if os.path.exists(self.corpus_file):
            try:
                with open(self.corpus_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # Convert timestamp strings back to datetime
                    for entry in data:
                        if isinstance(entry.get("timestamp"), str):
                            try:
                                entry["timestamp"] = datetime.fromisoformat(entry["timestamp"])
                            except:
                                pass
                    logger.info(f"Loaded {len(data)} corpus entries")
                    return data
            except Exception as e:
                logger.error(f"Error loading corpus: {e}")
        return []

    def clean_for_json(self, entry):
        if isinstance(entry, dict):
            return {k: self.clean_for_json(v) for k, v in entry.items()}
        elif isinstance(entry, list):
            return [self.clean_for_json(v) for v in entry]
        elif isinstance(entry, datetime):
            return entry.isoformat()
        else:
            return entry


    @log_and_time("Save Corpus")
    def save_corpus(self):
        """Save corpus to disk atomically"""
        tmp_file = self.corpus_file + ".tmp"
        try:
            with open(tmp_file, 'w', encoding='utf-8') as f:
                # Convert datetime to string for JSON
                data_to_save = []
                for entry in self.corpus:
                    entry_copy = entry.copy()
                    if isinstance(entry_copy.get("timestamp"), datetime):
                        entry_copy["timestamp"] = entry_copy["timestamp"].isoformat()
                    data_to_save.append(entry_copy)
                json.dump(self.clean_for_json(data_to_save), f, indent=2)
            os.replace(tmp_file, self.corpus_file)
            logger.debug(f"Saved {len(self.corpus)} entries to corpus")
        except Exception as e:
            logger.error(f"Error saving corpus: {e}")

    def add_entry(self, query: str, response: str, tags: List[str] = None):
        """Add a new interaction to corpus"""
        entry = {
            "query": query,
            "response": response,
            "timestamp": datetime.now(),
            "tags": tags or []
        }
        self.corpus.append(entry)

        # Auto-summarize every 20 non-summary entries
        real_entries = [e for e in self.corpus if "@summary" not in e.get("tags", [])]
        if len(real_entries) % 20 == 0 and len(real_entries) > 0:
            self._create_summary(real_entries[-20:])

        # Trim if too large
        if len(self.corpus) > 500:
            self.corpus = self.corpus[-500:]

        self.save_corpus()

    def _create_summary(self, entries: List[Dict]):
        """Create a summary node for the given entries"""
        from config.config import DEFAULT_SUMMARY_PROMPT_HEADER

        summary_lines = []
        for e in entries[:20]:
            if e.get('response', '').strip():
                q = e.get('query', '[no query]')[:50]
                r = e.get('response', '')[:60]
                summary_lines.append(f"Q: {q}... → A: {r}...")

        if summary_lines:
            summary_entry = {
                "query": "[SUMMARY NODE]",
                "response": DEFAULT_SUMMARY_PROMPT_HEADER + "\n".join(summary_lines),
                "timestamp": datetime.now(),
                "tags": ["@summary"]
            }
            self.corpus.append(summary_entry)
            logger.debug("Created summary node")

    def get_recent_memories(self, count: int = 3) -> List[Dict]:
        """Get most recent non-summary memories"""
        non_summary = [e for e in self.corpus if "@summary" not in e.get("tags", [])]
        return sorted(non_summary, key=lambda x: x.get('timestamp', datetime.min), reverse=True)[:count]

    def get_summaries(self, limit: int = 5) -> List[Dict]:
        """Get summary nodes - returns full dict not just content"""
        summaries = [e for e in self.corpus if "@summary" in e.get("tags", [])]
        # Sort by timestamp to get most recent summaries
        sorted_summaries = sorted(
            summaries,
            key=lambda x: x.get('timestamp', datetime.min),
            reverse=True
        )[:limit]

        # Return full dict for better integration
        return sorted_summaries
    def create_summary_now(self, entries_to_summarize: int = 20) -> Dict:
        """
        Manually trigger summary creation for the last N entries
        Returns the created summary entry
        """
        real_entries = [e for e in self.corpus if "@summary" not in e.get("tags", [])]

        if len(real_entries) < entries_to_summarize:
            entries_to_summarize = len(real_entries)

        if entries_to_summarize == 0:
            logger.warning("No entries to summarize")
            return None

        # Get the last N entries
        entries = real_entries[-entries_to_summarize:]

        # Create summary
        summary_lines = []
        for e in entries:
            if e.get('response', '').strip():
                q = e.get('query', '[no query]')[:50]
                r = e.get('response', '')[:60]
                timestamp = e.get('timestamp', datetime.now())

                # Format timestamp if it's a datetime
                if isinstance(timestamp, datetime):
                    time_str = timestamp.strftime("%H:%M")
                else:
                    time_str = ""

                summary_lines.append(f"[{time_str}] Q: {q}... → A: {r}...")

        if summary_lines:
            summary_entry = {
                "query": "[SUMMARY NODE]",
                "response": f"Summary of last {entries_to_summarize} exchanges:\n" + "\n".join(summary_lines),
                "timestamp": datetime.now(),
                "tags": ["@summary", f"covers_{entries_to_summarize}_entries"],
                "metadata": {
                    "start_time": entries[0].get('timestamp', datetime.now()),
                    "end_time": entries[-1].get('timestamp', datetime.now()),
                    "entry_count": entries_to_summarize
                }
            }

            self.corpus.append(summary_entry)
            self.save_corpus()
            logger.info(f"Created summary covering {entries_to_summarize} entries")

            return summary_entry

        return None
# memory/memory_coordinator.py
import asyncio
import uuid
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from utils.logging_utils import get_logger, log_and_time
from memory.storage.multi_collection_chroma_store import MultiCollectionChromaStore
from memory.memory_interface import HierarchicalMemorySystem
from processing.gate_system import MultiStageGateSystem

logger = get_logger("memory_coordinator")

class MemoryCoordinator:
    def __init__(self,
                 corpus_manager,
                 chroma_store: MultiCollectionChromaStore,
                 hierarchical_memory: Optional[HierarchicalMemorySystem] = None,
                 gate_system: Optional[MultiStageGateSystem] = None):
        self.corpus_manager = corpus_manager
        self.chroma_store = chroma_store
        self.hierarchical_memory = hierarchical_memory
        self.gate_system = gate_system

    @log_and_time("Retrieve Relevant Memories")
    async def retrieve_relevant_memories(self, query: str, config: Dict) -> Dict:
        logger.debug(f"Retrieving memories for query: {query[:50]}...")

        very_recent = self.corpus_manager.get_recent_memories(config.get('recent_count', 3))
        semantic_memories = await self._get_semantic_memories(query, config.get('semantic_count', 30))
        hierarchical_memories = []

        if self.hierarchical_memory:
            try:
                hierarchical_memories = await self.hierarchical_memory.retrieve_relevant_memories(
                    query,
                    max_memories=config.get('hierarchical_count', 15)
                )
            except Exception as e:
                logger.error(f"Error getting hierarchical memories: {e}")

        combined = await self._combine_memories(
            very_recent,
            semantic_memories,
            hierarchical_memories,
            query,
            config
        )

        return {
            'memories': combined,
            'counts': {
                'very_recent': len(very_recent),
                'semantic': len(semantic_memories),
                'hierarchical': len(hierarchical_memories),
                'final': len(combined)
            }
        }

    async def _get_semantic_memories(self, query: str, n_results: int = 30) -> List[Dict]:
        """Retrieve semantic memory chunks using the new multi-collection store"""
        try:
            results = self.chroma_store.search_all(query, n_results_per_type=n_results)
            semantic_chunks = results.get("semantic", []) + results.get("facts", [])

            memories = []
            for item in semantic_chunks:
                meta = item.get('metadata', {})
                memories.append({
                    'query': meta.get('query', item['content'][:100]),
                    'response': meta.get('response', ""),
                    'timestamp': meta.get('timestamp', datetime.now()),
                    'source': 'semantic',
                    'relevance_score': item.get('relevance_score', 0.5)
                })

            return memories

        except Exception as e:
            logger.error(f"Error in semantic memory retrieval: {e}")
            return []

    async def _combine_memories(self, very_recent, semantic, hierarchical, query, config) -> List[Dict]:
        combined = []
        seen = set()

        for mem in very_recent:
            key = self._get_memory_key(mem)
            if key not in seen:
                mem['source'] = 'very_recent'
                mem['gated'] = False
                combined.append(mem)
                seen.add(key)

        candidates = []

        for mem in semantic:
            key = self._get_memory_key(mem)
            if key not in seen:
                candidates.append(mem)

        for h in hierarchical:
            if isinstance(h, dict) and 'memory' in h:
                mem = self._format_hierarchical_memory(h['memory'])
                key = self._get_memory_key(mem)
                if key not in seen:
                    mem['relevance_score'] = h.get('final_score', 0.5)
                    candidates.append(mem)

        if self.gate_system and candidates:
            gated = await self._gate_memories(query, candidates)
            for mem in gated:
                mem['gated'] = True
                combined.append(mem)
        else:
            for mem in candidates[:config.get('max_memories', 10)]:
                mem['gated'] = False
                combined.append(mem)

        return combined

    async def _gate_memories(self, query: str, memories: List[Dict]) -> List[Dict]:
        try:
            chunks = [{
                "content": f"User: {m['query']}\nAssistant: {m['response']}",
                "metadata": {"timestamp": m.get("timestamp", datetime.now())}
            } for m in memories]

            filtered = await self.gate_system.filter_memories(query, chunks)

            gated = []
            for chunk in filtered:
                parts = chunk["content"].split("\nAssistant: ", 1)
                if len(parts) == 2:
                    gated.append({
                        'query': parts[0].replace("User: ", "").strip(),
                        'response': parts[1].strip(),
                        'timestamp': chunk["metadata"].get("timestamp", datetime.now())
                    })
            return gated

        except Exception as e:
            logger.error(f"Error during memory gating: {e}")
            return memories[:5]

    def _get_memory_key(self, memory: Dict) -> str:
        q = memory.get('query', '')[:50]
        r = memory.get('response', '')[:50]
        return f"{q}_{r}"

    def _format_hierarchical_memory(self, memory) -> Dict:
        parts = memory.content.split('\nAssistant: ')
        if len(parts) == 2:
            return {
                'query': parts[0].replace("User: ", "").strip(),
                'response': parts[1].strip(),
                'timestamp': memory.timestamp,
                'source': 'hierarchical'
            }
        return {
            'query': memory.content[:100],
            'response': "[Could not parse response]",
            'timestamp': memory.timestamp,
            'source': 'hierarchical'
        }

    async def store_interaction(self, query: str, response: str, tags: List[str] = None):
        self.corpus_manager.add_entry(query, response, tags)
        logger.debug("[MEMORY] store_interaction was called.")
        try:
            metadata = {
                "timestamp": datetime.now().isoformat(),
                "query": query,
                "response": response,
                "tags": ",".join(tags or [])
            }
            logger.debug(f"[MemoryCoordinator] Attempting to store memory:\nQuery: {query[:100]}\nResponse: {response[:100]}")
            self.chroma_store.add_conversation_memory(query, response, metadata)
            logger.debug("[MemoryCoordinator] ✅ Memory successfully stored")
        except Exception as e:
            logger.error(f"Error storing in ChromaDB: {e}")

        if self.hierarchical_memory:
            try:
                logger.debug(f"[MemoryCoordinator] Attempting to store memory:\nQuery: {query[:100]}\nResponse: {response[:100]}")

                await self.hierarchical_memory.store_interaction(query, response, tags)
                logger.debug("[MemoryCoordinator] ✅ Memory successfully stored")
            except Exception as e:
                logger.error(f"Error storing in hierarchical memory: {e}")

    def get_summaries(self, limit: int = 3) -> List[Dict]:
        summaries = self.corpus_manager.get_summaries(limit)
        return [{
            'content': s.get('response', ''),
            'timestamp': s.get('timestamp', datetime.now()),
            'type': 'summary',
            'tags': s.get('tags', [])
        } for s in summaries]

    def get_dreams(self, limit: int = 2) -> List[str]:
        if hasattr(self.corpus_manager, 'get_dreams'):
            dreams = self.corpus_manager.get_dreams(limit)
            return [{
                'content': d,
                'timestamp': datetime.now(),
                'source': 'dream'
            } for d in dreams]
        return []


    async def get_memories(self, query: str, limit: int = 20) -> List[Dict]:
        """Unified call for prompt builder to fetch top memories"""
        config = {
            'recent_count': 3,
            'semantic_count': 30,
            'hierarchical_count': 15,
            'max_memories': limit
        }
        results = await self.retrieve_relevant_memories(query, config)
        return results.get('memories', [])

import os
import json
import re
import uuid
from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Dict, List, Optional
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

from memory.storage.multi_collection_chroma_store import MultiCollectionChromaStore
from utils.logging_utils import get_logger, log_and_time
from processing.gate_system import MultiStageGateSystem
from config.config import MEM_NO, MEM_IMPORTANCE_SCORE, CHILD_MEM_LIMIT, GATE_REL_THRESHOLD

logger = get_logger("memory_interface")
logger.debug("Memory_Interface.py is alive")


class MemoryInterface(ABC):
    @abstractmethod
    def store(self, content: Dict) -> str:
        """Store a memory object. Returns memory ID."""
        pass

    @abstractmethod
    def retrieve(self, query: str, k: int = 5) -> List[Dict]:
        """Retrieve relevant memory chunks."""
        pass

    @abstractmethod
    def summarize(self) -> str:
        """Return a high-level summary."""
        pass


class MemoryType(Enum):
    EPISODIC = "episodic"      # Individual interactions
    SEMANTIC = "semantic"       # Extracted facts and knowledge
    PROCEDURAL = "procedural"   # How-to knowledge, patterns
    SUMMARY = "summary"         # Compressed episodic memories
    META = "meta"              # Memories about memory patterns


@dataclass
class MemoryNode:
    """Individual memory unit in the hierarchy"""
    id: str
    content: str
    type: MemoryType
    timestamp: datetime
    access_count: int = 0
    last_accessed: datetime = field(default_factory=datetime.now)
    importance_score: float = 0.5
    decay_rate: float = 0.1
    parent_id: Optional[str] = None
    child_ids: List[str] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)
    embeddings: Optional[List[float]] = None
    metadata: Dict = field(default_factory=dict)

    def to_dict(self):
        safe_metadata = {
            k: (
                json.dumps(v) if isinstance(v, (list, dict)) else v
            )
            for k, v in self.metadata.items()
        }

        return {
            "id": self.id,
            "content": self.content,
            "metadata": {
                "type": self.type.value,
                "timestamp": self.timestamp.isoformat(),
                "tags": json.dumps(self.tags),
                **safe_metadata
            }
        }

    @classmethod
    def from_dict(cls, data: Dict):
        """Reconstruct MemoryNode from dictionary"""
        metadata = data.get('metadata', {})

        # Parse tags
        tags = metadata.get('tags', '[]')
        if isinstance(tags, str):
            try:
                tags = json.loads(tags)
            except:
                tags = []

        # Parse timestamp
        timestamp_str = metadata.get('timestamp', '')
        try:
            timestamp = datetime.fromisoformat(timestamp_str)
        except:
            timestamp = datetime.now()

        # Get memory type
        mem_type = MemoryType(metadata.get('type', 'semantic'))

        return cls(
            id=data.get('id', str(uuid.uuid4())),
            content=data.get('content', ''),
            type=mem_type,
            timestamp=timestamp,
            tags=tags,
            metadata=metadata
        )


class HierarchicalMemorySystem:
    def __init__(
        self,
        model_manager,
        chroma_store: MultiCollectionChromaStore,
        storage_dir="hierarchical_memory",
        embed_model=None,
        cosine_threshold=0.45
    ):
        self.model_manager = model_manager
        self.storage_dir = storage_dir
        self.chroma_store = chroma_store
        self.memories: Dict[str, MemoryNode] = {}
        self.hierarchy: Dict[str, List[str]] = defaultdict(list)
        self.type_index: Dict[MemoryType, List[str]] = defaultdict(list)
        self.tag_index: Dict[str, List[str]] = defaultdict(list)
        self.embed_model = embed_model or SentenceTransformer("all-MiniLM-L6-v2")
        self.episodic_memory = []
        self.cosine_threshold = cosine_threshold

        # Config
        self.consolidation_threshold = 10
        self.importance_threshold = 0.7
        self.decay_interval = timedelta(days=1)

        # Initialize from ChromaDB collections
        self._load_from_chroma()

        # Semantic gating
        self.semantic_gate = MultiStageGateSystem(self.model_manager, cosine_threshold)

        # Ensure disk persistence directory
        os.makedirs(self.storage_dir, exist_ok=True)
        self.load_memories()

        logger.debug(f"Initialized HierarchicalMemorySystem with {len(self.memories)} memories")

    def _load_from_chroma(self):
        """Load memories from all ChromaDB collections using a full retrieval."""
        if not self.chroma_store:
            logger.warning("No ChromaDB store provided")
            return

        try:
            # Define collection mappings
            collection_to_memory_type = {
                'conversations': MemoryType.EPISODIC,
                'semantic': MemoryType.SEMANTIC,
                'summaries': MemoryType.SUMMARY,
                'facts': MemoryType.SEMANTIC,
                'wiki': MemoryType.SEMANTIC
            }

            total_loaded = 0

            # Load from each collection
            for collection_key, memory_type in collection_to_memory_type.items():
                try:
                    # Use the new get_all_from_collection method instead of search
                    collection_results = self.chroma_store.get_all_from_collection(collection_key)

                    logger.debug(f"Loading {len(collection_results)} items from {collection_key}")

                    for item in collection_results:
                        try:
                            # Create MemoryNode from result
                            node = MemoryNode(
                                id=item['id'],
                                content=item['content'],
                                type=memory_type,
                                timestamp=datetime.fromisoformat(
                                    item['metadata'].get('timestamp', datetime.now().isoformat())
                                ),
                                tags=self._parse_tags(item['metadata'].get('tags', '')),
                                metadata=item['metadata']
                            )

                            # Store in memory system
                            self.memories[node.id] = node
                            self.type_index[memory_type].append(node.id)

                            for tag in node.tags:
                                self.tag_index[tag].append(node.id)

                            if memory_type == MemoryType.EPISODIC:
                                self.episodic_memory.append(node.content)

                            total_loaded += 1

                        except Exception as e:
                            logger.error(f"Error loading memory {item.get('id', 'unknown')}: {e}")

                except Exception as e:
                    logger.error(f"Error loading collection {collection_key}: {e}")

            logger.info(f"Loaded {total_loaded} memories from ChromaDB")

        except Exception as e:
            logger.error(f"Error loading from ChromaDB: {e}")

    def _parse_tags(self, tags_str: str) -> List[str]:
        """Parse tags from string format"""
        if not tags_str:
            return []
        if isinstance(tags_str, list):
            return tags_str
        try:
            return json.loads(tags_str)
        except:
            # Try comma-separated
            return [t.strip() for t in tags_str.split(',') if t.strip()]

    @log_and_time("store interaction")
    async def store_interaction(self, query: str, response: str, tags: List[str] = None) -> str:
        """Store a new interaction and trigger hierarchical processing"""
        # Create episodic memory
        memory_id = str(uuid.uuid4())
        content = f"User: {query}\nAssistant: {response}"
        metadata = {"query": query, "response": response}

        # Calculate importance score
        importance = await self._calculate_importance_heuristic(content)

        memory = MemoryNode(
            id=memory_id,
            content=content,
            type=MemoryType.EPISODIC,
            timestamp=datetime.now(),
            importance_score=importance,
            tags=tags or [],
            metadata=metadata
        )

        # Store in local memory
        self.memories[memory_id] = memory
        self.type_index[MemoryType.EPISODIC].append(memory_id)
        self.episodic_memory.append(content)

        # Update tag index
        for tag in memory.tags:
            self.tag_index[tag].append(memory_id)

        # Store in ChromaDB
        self.chroma_store.add_conversation_memory(query, response, metadata)

        # Extract semantic knowledge
        semantic_memories = await self._extract_semantic_knowledge_heuristic(query, response)

        for sem_mem in semantic_memories:
            # Store in local memory
            self.memories[sem_mem.id] = sem_mem
            self.type_index[MemoryType.SEMANTIC].append(sem_mem.id)
            sem_mem.parent_id = memory_id
            memory.child_ids.append(sem_mem.id)

            # Store in ChromaDB
            self.chroma_store.add_semantic_chunk({
                'content': sem_mem.content,
                'source': 'conversation_extraction',
                'type': 'fact',
                'importance': sem_mem.importance_score
            })

        # Check for consolidation
        if len(self.type_index[MemoryType.EPISODIC]) % self.consolidation_threshold == 0:
            await self._consolidate_memories_heuristic()

        # Save to disk
        self.save_memories()

        return memory_id

    @log_and_time("Retrieve Memory")
    async def retrieve_relevant_memories(self, query: str, max_memories: int = 10) -> List[Dict]:
        """Retrieve memories relevant to the query"""
        logger.debug(f"\n[DEBUG] Total memories in system: {len(self.memories)}")
        logger.debug(f"Memory types: {[(t.value, len(ids)) for t, ids in self.type_index.items() if ids]}")

        all_relevant = []

        # 1. Search ChromaDB collections
        if self.chroma_store:
            try:
                # Search all collections
                chroma_results = self.chroma_store.search_all(query, n_results_per_type=10)

                for collection_key, results in chroma_results.items():
                    for result in results:
                        # Check if we have this memory locally
                        mem_id = result['id']
                        if mem_id in self.memories:
                            memory = self.memories[mem_id]
                            all_relevant.append({
                                'id': mem_id,
                                'memory': memory,
                                'relevance_score': result.get('relevance_score', 0.5),
                                'content': memory.content,
                                'source': f'chroma_{collection_key}'
                            })
                        else:
                            # Create temporary memory node for ChromaDB-only results
                            memory = MemoryNode(
                                id=mem_id,
                                content=result['content'],
                                type=self._get_memory_type_for_collection(collection_key),
                                timestamp=datetime.now(),
                                metadata=result.get('metadata', {})
                            )
                            all_relevant.append({
                                'id': mem_id,
                                'memory': memory,
                                'relevance_score': result.get('relevance_score', 0.5),
                                'content': result['content'],
                                'source': f'chroma_{collection_key}'
                            })

            except Exception as e:
                logger.error(f"Error searching ChromaDB: {e}")

        # 2. Search local memories using embeddings
        if self.memories:
            local_results = await self._search_local_memories(query, top_k=20)
            all_relevant.extend(local_results)

        # 3. Remove duplicates (keep highest score)
        seen = {}
        for mem in all_relevant:
            mem_id = mem['id']
            if mem_id not in seen or mem['relevance_score'] > seen[mem_id]['relevance_score']:
                seen[mem_id] = mem

        relevant_memories = list(seen.values())

        # 4. Apply temporal decay and importance weighting
        scored_memories = self._apply_temporal_decay(relevant_memories)

        # 5. Sort by final score
        sorted_memories = sorted(scored_memories, key=lambda x: x['final_score'], reverse=True)

        # 6. Update access counts
        for mem in sorted_memories[:max_memories]:
            self._update_access(mem['id'])

        logger.debug(f"Retrieved {len(sorted_memories[:max_memories])} memories")
        return sorted_memories[:max_memories]

    async def _search_local_memories(self, query: str, top_k: int = 20) -> List[Dict]:
        """Search local memories using embeddings"""
        if not self.memories:
            return []

        try:
            # Encode query
            query_emb = self.embed_model.encode(query, convert_to_numpy=True)

            # Get all memory contents
            memory_ids = list(self.memories.keys())
            contents = [self.memories[mid].content[:500] for mid in memory_ids]

            # Encode all memories
            memory_embs = self.embed_model.encode(contents, convert_to_numpy=True, batch_size=32)

            # Calculate similarities
            similarities = cosine_similarity([query_emb], memory_embs)[0]

            # Get top results
            top_indices = np.argsort(similarities)[-top_k:][::-1]

            results = []
            for idx in top_indices:
                if similarities[idx] >= self.cosine_threshold:
                    mem_id = memory_ids[idx]
                    memory = self.memories[mem_id]
                    results.append({
                        'id': mem_id,
                        'memory': memory,
                        'relevance_score': float(similarities[idx]),
                        'content': memory.content,
                        'source': 'local_search'
                    })

            return results

        except Exception as e:
            logger.error(f"Error in local memory search: {e}")
            return []

    def _get_memory_type_for_collection(self, collection_key: str) -> MemoryType:
        """Map collection key to memory type"""
        mapping = {
            'conversations': MemoryType.EPISODIC,
            'semantic': MemoryType.SEMANTIC,
            'summaries': MemoryType.SUMMARY,
            'facts': MemoryType.SEMANTIC,
            'wiki': MemoryType.SEMANTIC
        }
        return mapping.get(collection_key, MemoryType.SEMANTIC)

    @log_and_time("Calculate mem importance heuristic")
    async def _calculate_importance_heuristic(self, content: str) -> float:
        """Calculate importance score using simple heuristics"""
        score = 0.5

        # Boost for longer content
        if len(content) > 200:
            score += 0.1

        # Boost for questions
        if '?' in content:
            score += 0.1

        # Boost for certain keywords
        important_keywords = ['important', 'remember', 'note', 'key', 'critical', 'essential']
        if any(kw in content.lower() for kw in important_keywords):
            score += 0.2

        return min(score, 1.0)

    @log_and_time("Extract semantic knowledge heuristic")
    async def _extract_semantic_knowledge_heuristic(self, query: str, response: str) -> List[MemoryNode]:
        """Extract semantic facts using simple pattern matching"""
        semantic_memories = []

        # Look for facts pattern
        fact_patterns = [
            r'(\w+)\s+(?:is|are|was|were)\s+(.+?)(?:\.|,|;|$)',
            r'(?:I|you|we)\s+(?:like|love|hate|prefer)\s+(.+?)(?:\.|,|;|$)',
            r'(?:my|your|our)\s+(\w+)\s+(?:is|are)\s+(.+?)(?:\.|,|;|$)'
        ]

        combined_text = f"{query} {response}"
        facts = []

        for pattern in fact_patterns:
            matches = re.findall(pattern, combined_text, re.IGNORECASE)
            for match in matches[:3]:
                if isinstance(match, tuple):
                    fact = ' '.join(match)
                else:
                    fact = match
                if len(fact) > 10 and len(fact) < 200:
                    facts.append(fact.strip())

        # Create memory nodes for unique facts
        seen_facts = set()
        for fact in facts[:5]:
            if fact.lower() not in seen_facts:
                seen_facts.add(fact.lower())
                sem_id = str(uuid.uuid4())
                semantic_memories.append(MemoryNode(
                    id=sem_id,
                    content=fact,
                    type=MemoryType.SEMANTIC,
                    timestamp=datetime.now(),
                    importance_score=0.6,
                    tags=["extracted_fact"]
                ))

        return semantic_memories

    @log_and_time("Consolidate Memories Heuristic")
    async def _consolidate_memories_heuristic(self):
        """Consolidate recent episodic memories into summaries"""
        recent_episodic = self.type_index[MemoryType.EPISODIC][-self.consolidation_threshold:]

        if len(recent_episodic) < self.consolidation_threshold:
            return

        # Gather content for summarization
        contents = []
        for mem_id in recent_episodic:
            if mem_id in self.memories:
                contents.append(self.memories[mem_id].content)

        # Simple summarization
        summary_lines = []
        for content in contents[:5]:
            user_match = re.search(r'User:\s*(.+?)(?=\nAssistant:|$)', content)
            assistant_match = re.search(r'Assistant:\s*(.+?)(?=\n|$)', content)

            if user_match and assistant_match:
                q = user_match.group(1).strip()[:50]
                a = assistant_match.group(1).strip()[:50]
                summary_lines.append(f"Q: {q}... A: {a}...")

        if summary_lines:
            summary = "Summary of recent interactions:\n" + "\n".join(summary_lines)

            summary_id = str(uuid.uuid4())
            summary_node = MemoryNode(
                id=summary_id,
                content=summary,
                type=MemoryType.SUMMARY,
                timestamp=datetime.now(),
                importance_score=0.8,
                tags=["consolidated_summary"]
            )

            # Store locally
            self.memories[summary_id] = summary_node
            self.type_index[MemoryType.SUMMARY].append(summary_id)

            # Store in ChromaDB
            self.chroma_store.add_summary(
                summary=summary,
                period=f"consolidation_{datetime.now().strftime('%Y%m%d_%H%M')}",
                metadata={'source': 'auto_consolidation'}
            )

            # Link child memories
            for mem_id in recent_episodic:
                if mem_id in self.memories:
                    self.memories[mem_id].parent_id = summary_id
                    summary_node.child_ids.append(mem_id)

    def _apply_temporal_decay(self, memories: List[Dict]) -> List[Dict]:
        """Apply temporal decay to memory scores"""
        now = datetime.now()

        for mem_dict in memories:
            memory = mem_dict['memory']

            # Calculate age in days
            age_days = (now - memory.timestamp).days

            # Apply decay function
            decay_factor = 1.0 / (1.0 + memory.decay_rate * age_days)

            # Boost recently accessed memories
            access_recency = (now - memory.last_accessed).days
            access_boost = 1.0 if access_recency < 1 else 1.0 / (1.0 + 0.1 * access_recency)

            # Calculate final score
            mem_dict['final_score'] = (
                mem_dict['relevance_score'] *
                memory.importance_score *
                decay_factor *
                access_boost
            )

        return memories

    def _update_access(self, memory_id: str):
        """Update access count and timestamp"""
        if memory_id in self.memories:
            self.memories[memory_id].access_count += 1
            self.memories[memory_id].last_accessed = datetime.now()

    def save_memories(self):
        """Save memories to disk"""
        data = {
            'memories': {
                id: {
                    'id': m.id,
                    'content': m.content,
                    'type': m.type.value,
                    'timestamp': m.timestamp.isoformat(),
                    'access_count': m.access_count,
                    'last_accessed': m.last_accessed.isoformat(),
                    'importance_score': m.importance_score,
                    'decay_rate': m.decay_rate,
                    'parent_id': m.parent_id,
                    'child_ids': m.child_ids,
                    'tags': m.tags,
                    'metadata': m.metadata
                }
                for id, m in self.memories.items()
            },
            'hierarchy': dict(self.hierarchy),
            'type_index': {k.value: v for k, v in self.type_index.items()},
            'tag_index': dict(self.tag_index)
        }

        save_path = os.path.join(self.storage_dir, 'hierarchical_memories.json')
        with open(save_path, 'w') as f:
            json.dump(data, f, indent=2)

    @log_and_time("Load Memories")
    def load_memories(self):
        """Load memories from disk"""
        save_path = os.path.join(self.storage_dir, 'hierarchical_memories.json')

        if os.path.exists(save_path):
            try:
                with open(save_path, 'r') as f:
                    data = json.load(f)

                for id, mem_data in data.get('memories', {}).items():
                    if id not in self.memories:  # Don't overwrite ChromaDB loaded memories
                        try:
                            memory = MemoryNode(
                                id=mem_data['id'],
                                content=mem_data['content'],
                                type=MemoryType(mem_data['type']),
                                timestamp=datetime.fromisoformat(mem_data['timestamp']),
                                access_count=mem_data['access_count'],
                                last_accessed=datetime.fromisoformat(mem_data['last_accessed']),
                                importance_score=mem_data['importance_score'],
                                decay_rate=mem_data['decay_rate'],
                                parent_id=mem_data.get('parent_id'),
                                child_ids=mem_data.get('child_ids', []),
                                tags=mem_data.get('tags', []),
                                metadata=mem_data.get('metadata', {})
                            )
                            self.memories[id] = memory
                        except Exception as e:
                            logger.debug(f"[LOAD ERROR] Could not load memory ID {id}: {e}")

                # Reconstruct indices
                self.hierarchy = defaultdict(list, data.get('hierarchy', {}))
                for type_str, ids in data.get('type_index', {}).items():
                    self.type_index[MemoryType(type_str)] = ids
                self.tag_index = defaultdict(list, data.get('tag_index', {}))

            except Exception as e:
                logger.error(f"[ERROR] Failed to load hierarchical memory file: {e}")

    def get_summaries(self, query: str = None, limit: int = 5):
        """Get summaries from ChromaDB"""
        if self.chroma_store:
            return self.chroma_store.search_conversations(query or "summary", n_results=limit)
        return []

    def get_dreams(self, query: str = None, limit: int = 5):
        """Get dreams/meta memories"""
        # This would be implemented based on your dream generation logic
        return []

# memory/storage/chroma_store_multi.py
# Multi-collection ChromaDB storage system

import chromadb
from chromadb.config import Settings
import hashlib
from datetime import datetime
import json
from typing import List, Dict, Optional
from sentence_transformers import SentenceTransformer
import logging

logger = logging.getLogger(__name__)

class MultiCollectionChromaStore:
    """ChromaDB store with separate collections for different memory types"""

    # Define collection names and their purposes
    COLLECTIONS = {
        'conversations': {
            'name': 'conversation-memory',
            'description': 'User-assistant conversation history',
            'embedding_model': 'all-MiniLM-L6-v2'
        },
        'wiki': {
            'name': 'wiki-knowledge',
            'description': 'Wikipedia article chunks',
            'embedding_model': 'all-MiniLM-L6-v2'
        },
        'semantic': {
            'name': 'semantic-chunks',
            'description': 'Semantically chunked long-form content',
            'embedding_model': 'all-MiniLM-L6-v2'
        },
        'summaries': {
            'name': 'conversation-summaries',
            'description': 'Periodic summaries of conversations',
            'embedding_model': 'all-MiniLM-L6-v2'
        },
        'facts': {
            'name': 'extracted-facts',
            'description': 'Important facts and information extracted from conversations',
            'embedding_model': 'all-MiniLM-L6-v2'
        }
    }

    def __init__(self, persist_directory: str = "data/chroma_multi"):
        self.persist_directory = persist_directory
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )

        # Initialize embedding models (could use different ones per collection)
        self.embedders = {}
        self.collections = {}

        # Initialize all collections
        self._initialize_collections()

    def _initialize_collections(self):
        """Initialize all collection types"""
        for key, config in self.COLLECTIONS.items():
            try:
                # Try to get existing collection
                collection = self.client.get_collection(config['name'])
                logger.info(f"Loaded existing collection: {config['name']} ({collection.count()} documents)")
            except:
                # Create new collection
                collection = self.client.create_collection(
                    name=config['name'],
                    metadata={
                        "description": config['description'],
                        "embedding_model": config['embedding_model'],
                        "created_at": datetime.now().isoformat()
                    }
                )
                logger.info(f"Created new collection: {config['name']}")

            self.collections[key] = collection

            # Initialize embedder for this collection (lazy load)
            if config['embedding_model'] not in self.embedders:
                self.embedders[config['embedding_model']] = SentenceTransformer(config['embedding_model'])

    def _generate_id(self, content: str, collection_type: str) -> str:
        """Generate unique ID for a memory"""
        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S%f")[:-3]
        return f"{collection_type}_{content_hash}_{timestamp}"

    def add_conversation_memory(self, query: str, response: str, metadata: Dict = None) -> str:
        """Add a conversation turn to memory"""
        content = f"User: {query}\nAssistant: {response}"
        memory_id = self._generate_id(content, "conv")

        metadata = metadata or {}
        metadata.update({
            "timestamp": datetime.now().isoformat(),
            "query": query[:200],  # Store preview
            "response": response[:200],
            "type": "conversation",
            "turn_length": len(query) + len(response)
        })

        self.collections['conversations'].add(
            ids=[memory_id],
            documents=[content],
            metadatas=[metadata]
        )

        logger.debug(f"Added conversation memory: {memory_id}")
        return memory_id

    def add_wiki_chunk(self, chunk: Dict) -> str:
        """Add a Wikipedia chunk"""
        content = f"{chunk.get('title', '')} {chunk.get('text', '')}"
        memory_id = self._generate_id(content, "wiki")

        metadata = {
            "title": chunk.get('title', 'Unknown'),
            "article_id": chunk.get('id', 'unknown'),
            "chunk_index": chunk.get('chunk_index', 0),
            "timestamp": datetime.now().isoformat(),
            "type": "wiki"
        }

        self.collections['wiki'].add(
            ids=[memory_id],
            documents=[content],
            metadatas=[metadata]
        )

        return memory_id

    def add_semantic_chunk(self, chunk: Dict) -> str:
        """Add a semantic chunk from long-form content"""
        content = chunk.get('content', chunk.get('text', ''))
        memory_id = self._generate_id(content, "sem")

        metadata = {
            "source": chunk.get('source', 'unknown'),
            "chunk_type": chunk.get('type', 'paragraph'),
            "importance": chunk.get('importance', 0.5),
            "timestamp": datetime.now().isoformat(),
            "type": "semantic"
        }

        self.collections['semantic'].add(
            ids=[memory_id],
            documents=[content],
            metadatas=[metadata]
        )

        return memory_id

    def add_summary(self, summary: str, period: str, metadata: Dict = None) -> str:
        """Add a conversation summary"""
        memory_id = self._generate_id(summary, "summ")

        metadata = metadata or {}
        metadata.update({
            "period": period,  # e.g., "2024-01-15_afternoon"
            "timestamp": datetime.now().isoformat(),
            "type": "summary",
            "length": len(summary)
        })

        self.collections['summaries'].add(
            ids=[memory_id],
            documents=[summary],
            metadatas=[metadata]
        )

        return memory_id

    def add_fact(self, fact: str, source: str, confidence: float = 1.0) -> str:
        """Add an extracted fact"""
        memory_id = self._generate_id(fact, "fact")

        metadata = {
            "source": source,
            "confidence": confidence,
            "timestamp": datetime.now().isoformat(),
            "type": "fact"
        }

        self.collections['facts'].add(
            ids=[memory_id],
            documents=[fact],
            metadatas=[metadata]
        )

        return memory_id

    def search_conversations(self, query: str, n_results: int = 5) -> List[Dict]:
        """Search conversation memories"""
        results = self.collections['conversations'].query(
            query_texts=[query],
            n_results=n_results
        )

        return self._format_results(results, 'conversations')

    def search_wiki(self, query: str, n_results: int = 5) -> List[Dict]:
        """Search Wikipedia chunks"""
        results = self.collections['wiki'].query(
            query_texts=[query],
            n_results=n_results
        )

        return self._format_results(results, 'wiki')

    def search_all(self, query: str, n_results_per_type: int = 3) -> Dict[str, List[Dict]]:
        """Search across all collections"""
        all_results = {}

        for key, collection in self.collections.items():
            try:
                results = collection.query(
                    query_texts=[query],
                    n_results=n_results_per_type
                )
                all_results[key] = self._format_results(results, key)
            except Exception as e:
                logger.error(f"Error searching {key}: {e}")
                all_results[key] = []

        return all_results

    def _format_results(self, results: Dict, collection_type: str) -> List[Dict]:
        """Format ChromaDB results into consistent structure"""
        formatted = []

        if not results['ids'] or not results['ids'][0]:
            return formatted

        for i, (doc_id, doc, meta, distance) in enumerate(zip(
            results['ids'][0],
            results['documents'][0],
            results['metadatas'][0],
            results['distances'][0]
        )):
            formatted.append({
                'id': doc_id,
                'content': doc,
                'metadata': meta,
                'relevance_score': 1.0 / (1.0 + distance),  # Convert distance to similarity
                'collection': collection_type,
                'rank': i + 1
            })

        return formatted

    def get_collection_stats(self) -> Dict[str, Dict]:
        """Get statistics for all collections"""
        stats = {}

        for key, collection in self.collections.items():
            stats[key] = {
                'name': self.COLLECTIONS[key]['name'],
                'description': self.COLLECTIONS[key]['description'],
                'count': collection.count(),
                'embedding_model': self.COLLECTIONS[key]['embedding_model']
            }

        return stats

    def clear_collection(self, collection_key: str):
        """Clear a specific collection"""
        if collection_key in self.collections:
            # Delete and recreate the collection
            self.client.delete_collection(self.COLLECTIONS[collection_key]['name'])
            logger.info(f"Cleared collection: {collection_key}")

            # Reinitialize
            self._initialize_collections()

    def migrate_from_single_collection(self, old_collection_name: str = "assistant-memory"):
        """Migrate from single collection to multi-collection setup"""
        try:
            old_collection = self.client.get_collection(old_collection_name)
            all_data = old_collection.get()

            if not all_data['ids']:
                logger.info("No data to migrate")
                return

            migrated = {
                'conversations': 0,
                'wiki': 0,
                'summaries': 0,
                'facts': 0,
                'semantic': 0
            }

            for doc_id, doc, meta in zip(all_data['ids'], all_data['documents'], all_data['metadatas']):
                # Determine type based on content or metadata
                if 'User:' in doc and 'Assistant:' in doc:
                    self.add_conversation_memory(
                        query=doc.split('Assistant:')[0].replace('User:', '').strip(),
                        response=doc.split('Assistant:')[1].strip(),
                        metadata=meta
                    )
                    migrated['conversations'] += 1
                elif meta.get('type') == 'wiki' or 'title' in meta:
                    self.collections['wiki'].add(
                        ids=[doc_id],
                        documents=[doc],
                        metadatas=[meta]
                    )
                    migrated['wiki'] += 1
                elif '@summary' in meta.get('tags', []):
                    self.add_summary(doc, period="migrated", metadata=meta)
                    migrated['summaries'] += 1
                else:
                    # Default to semantic chunks
                    self.collections['semantic'].add(
                        ids=[doc_id],
                        documents=[doc],
                        metadatas=[meta]
                    )
                    migrated['semantic'] += 1

            logger.info(f"Migration complete: {migrated}")
            return migrated

        except Exception as e:
            logger.error(f"Migration failed: {e}")
            return None
# Import dependencies and config defaults
import logging
from utils.logging_utils import log_and_time, get_logger
# Use the root logger or create a child logger that will inherit handlers
logger = get_logger("main")
import re
import json
import os
from datetime import datetime, timedelta
from config.config import DEFAULT_MAX_TOKENS, DEFAULT_TEMPERATURE, DEFAULT_TOP_P, DEFAULT_TOP_K, OpenAPIKey, SYSTEM_PROMPT
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer
import time
import torch
from openai import OpenAI, AsyncOpenAI
import httpx
import os
import asyncio



# Set OpenAI API key for API calls
class ModelManager:
    """Manager class for handling both local and API-based language models."""

    def __init__(self, api_key: str = None):
        # Active model name (local or API)
        self.active_model_name = None
        # Dictionary of loaded local models
        self.models = {}
        self.allow_fallback = False  # Disable fallback to unknown API models
        # Dictionary of loaded tokenizers for local models
        self.tokenizers = {}
        # Dictionary mapping registered API models
        self.api_models = {}
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=self.api_key,   http_client=httpx.Client(
        timeout=httpx.Timeout(30.0),
        limits=httpx.Limits(
            max_connections=100,
            max_keepalive_connections=10
        ),
        headers={"Connection": "keep-alive"},
    )
    )
        self.async_client = AsyncOpenAI(api_key=self.api_key)
        self.default_model = "gpt-4-turbo"
        self.embed_model = SentenceTransformer("all-MiniLM-L6-v2")
    def get_context_limit(self):
        """Get the maximum context window based on active model."""
        if self.is_api_model(self.get_active_model_name()):
            # For API models (OpenAI), assume known context window
            # logger.debug(f" Using OpenAI model, assuming 128000 token context window.")
            return 128000  # Default GPT-4 Turbo context
        model = self.get_model()
        if model:
            return model.config.max_position_embeddings
        else:
            raise ValueError("[ERROR] No model loaded. Cannot determine context limit.")

    @log_and_time("Get Embedder")
    def get_embedder(self):
        return self.embed_model
    @log_and_time("Load Model")
    def load_model(self, model_name, model_path):
        """Load a local Huggingface model and tokenizer."""
        try:
            # Determine if using local files only
            local_files_only = model_path.startswith("./")
            # logger.debug( Loading local model '{model_name}' from '{model_path}'")

            # Load model
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                local_files_only=local_files_only,
                device_map="auto",
                trust_remote_code=True
            )

            # Load tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                local_files_only=local_files_only,
                trust_remote_code=True,
                use_fast=True
            )

            # Ensure tokenizer has a pad token set
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            # Set tokenizer max length to model context window
            tokenizer.model_max_length = model.config.max_position_embeddings

            # Store model and tokenizer
            self.models[model_name] = model
            self.tokenizers[model_name] = tokenizer

            # logger.debug( Successfully loaded model: {model.__class__.__name__}")
        except Exception as e:
            raise ValueError(f"Error loading model '{model_name}': {str(e)}")
    @log_and_time("Load OpenAI")
    def load_openai_model(self, model_name, api_model_name):
        """Register an OpenAI API model (no local loading)."""
        # logger.debug( Registering OpenAI model '{model_name}'")
        self.api_models[model_name] = api_model_name
    def close(self):
        """Gracefully close the HTTP client to avoid socket leak."""
        if hasattr(self.client, "_client"):
            self.client._client.close()
    async def aclose(self):
        if hasattr(self.client, "_client"):
            await self.client._client.aclose()
    def is_api_model(self, model_name):
        """Check if a given model is an API-based model."""
        return model_name in self.api_models

    @log_and_time("Generate with openAI")
    def generate_with_openai(self, prompt, model_name, system_prompt=None, max_tokens=None, temperature=None, top_p=None):
        """Generate text using OpenAI API, with global defaults fallback."""
        # Apply global defaults if not provided
        max_tokens = DEFAULT_MAX_TOKENS if max_tokens is None else max_tokens
        temperature = DEFAULT_TEMPERATURE if temperature is None else temperature
        top_p = DEFAULT_TOP_P if top_p is None else top_p

        try:
            # logger.debug(  Calling OpenAI API: {model_name}")
            response = self.client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt or SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            return f"[OpenAI API Error] {str(e)}"

    def switch_model(self, model_name):
        """Switch active model (local or API)."""
        self.active_model_name = model_name

    def get_model(self):
        """Return active local model instance (if any)."""
        return self.models.get(self.active_model_name)

    def get_tokenizer(self):
        """Return active tokenizer instance (if any)."""
        return self.tokenizers.get(self.active_model_name)

    def get_active_model_name(self):
        """Return the name of the currently active model."""
        return self.active_model_name

    @staticmethod
    def truncate_prompt(prompt, tokenizer, max_input_tokens, preserve_prefix="You are Daemon"):
        """Ensure prompt fits within model's input size (optional prefix preservation)."""
        if preserve_prefix in prompt:
            prefix_index = prompt.index(preserve_prefix)
            prefix = prompt[:prefix_index + len(preserve_prefix)]
            rest = prompt[prefix_index + len(preserve_prefix):]
        else:
            prefix = ""
            rest = prompt

        prefix_tokens = tokenizer.encode(prefix)
        rest_tokens = tokenizer.encode(rest)

        # No truncation needed
        if len(prefix_tokens) + len(rest_tokens) <= max_input_tokens:
            return prompt

        # logger.debug( Truncating prompt: {len(prefix_tokens) + len(rest_tokens)} → {max_input_tokens} tokens")

        # If prefix alone is too long, truncate from the end of entire prompt
        allowed_rest_tokens = max_input_tokens - len(prefix_tokens)
        if allowed_rest_tokens <= 0:
            print("[WARN] Prefix alone exceeds max input size.")
            return tokenizer.decode((prefix_tokens + rest_tokens)[-max_input_tokens:])

        # Truncate rest of prompt and return combined prompt
        truncated_rest_tokens = rest_tokens[-allowed_rest_tokens:]
        return tokenizer.decode(prefix_tokens + truncated_rest_tokens)



    @log_and_time("ModelManager Generate Call")
    def generate(self, prompt, model_name="gpt-4-turbo", max_tokens=None, temperature=None, top_p=None, top_k=None, no_repeat_ngram_size=None, pad_token_id=None, system_prompt=None):
        """Main generate function for both local and OpenAI models."""

        # IMPORTANT: Use the provided model_name if given, otherwise use active model
        logger.debug(f"[generate] Model received: {model_name}")
        logger.debug(f"[generate] Known local models: {list(self.models.keys())}")
        logger.debug(f"[generate] Known API models: {list(self.api_models.keys())}")

        target_model = model_name or self.active_model_name

        if not target_model:
            raise ValueError("No model specified. Pass model_name or use switch_model() first.")

        # Check if this is a local model FIRST
        if target_model in self.models:  # This is a local model
            logger.debug(f"Using local model: {target_model}")

            # Get the specific local model and tokenizer
            model = self.models[target_model]
            tokenizer = self.tokenizers[target_model]

            # Use defaults where needed
            max_tokens = 64 if max_tokens is None else max_tokens
            temperature = DEFAULT_TEMPERATURE if temperature is None else temperature
            top_p = DEFAULT_TOP_P if top_p is None else top_p
            top_k = DEFAULT_TOP_K if top_k is None else top_k

            # Check input length and truncate if needed
            tokens = tokenizer.encode(prompt)
            max_len = model.config.max_position_embeddings

            if len(tokens) > max_len:
                tokens = tokens[-max_len:]
                prompt = tokenizer.decode(tokens)

            # Prepare safe prompt for max input size
            context_limit = model.config.max_position_embeddings
            max_input_tokens = context_limit - max_tokens
            safe_prompt = self.truncate_prompt(prompt, tokenizer, max_input_tokens)

            # Tokenize and move inputs to model device
            inputs = tokenizer(safe_prompt, return_tensors="pt")
            inputs = {k: v.to(model.device) for k, v in inputs.items()}

            # Generate output with Huggingface model
            with torch.no_grad():
                start = time.time()
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=False,
                    num_beams=1,
                    pad_token_id=pad_token_id or tokenizer.pad_token_id
            )
                end = time.time()
                logger.debug(f"Local generation finished in {end - start:.2f} seconds")

            # Decode and return output text
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Remove the input prompt from the output
            return generated_text[len(safe_prompt):].strip()

        # API model generation path
        elif target_model in self.api_models:
            logger.debug(f"Using OpenAI model alias: {self.api_models[target_model]}")
            return self.generate_with_openai(
                prompt,
                self.api_models[target_model],
                system_prompt=system_prompt,
                max_tokens=max_tokens or 500,
                temperature=temperature or 0.7,
                top_p=top_p or 1.0
            )

        else:
            if not self.allow_fallback:
                raise ValueError(f"[ModelManager] Model '{target_model}' is not recognized as a local or registered API model. Fallback is disabled.")

            logger.warning(f"[ModelManager] Fallback triggered for unknown model: {target_model}")
            return self.generate_with_openai(
                prompt,
                target_model,
                system_prompt=system_prompt,
                max_tokens=max_tokens or 500,
                temperature=temperature or 0.7,
                top_p=top_p or 1.0
            )

    @log_and_time("ModelManager Generate Async")
    async def generate_async(self, prompt, raw=False, **kwargs):
        """Async wrapper for generation using the active model"""
        target_model = self.active_model_name  # No longer allows override
        logger.debug(f"[generate_async] Active model: {target_model}")
        logger.debug(f"[generate_async] Registered OpenAI models: {self.api_models}")
        logger.debug(f"[generate_async] Registered local models: {self.models}")
        self.switch_model("gpt-4-turbo")
        logger.debug(f"[generate_async] Forcing model to: {target_model}")


        if target_model in self.models:
            return await asyncio.to_thread(
                self.generate, prompt, model_name=target_model, **kwargs
            )

        elif target_model in self.api_models:
            try:
                logger.debug(f"[ModelManager] Using OpenAI async model: {target_model}")

                if raw:
                    # Bypass all prompt-building logic
                    messages = [{"role": "user", "content": prompt}]
                else:
                    # Inject system prompt as usual
                    if kwargs.get('system_prompt', SYSTEM_PROMPT) is not None:
                        messages = [
                            {"role": "system", "content": kwargs.get('system_prompt', SYSTEM_PROMPT)},
                            {"role": "user", "content": prompt}
                        ]
                    else:
                        messages = [{"role": "user", "content": prompt}]

                stream = await self.async_client.chat.completions.create(
                    model=self.api_models[target_model],
                    messages=messages,
                    max_tokens=kwargs.get('max_tokens', DEFAULT_MAX_TOKENS),
                    temperature=kwargs.get('temperature', DEFAULT_TEMPERATURE),
                    top_p=kwargs.get('top_p', DEFAULT_TOP_P),
                    stream=True
                )
                return stream

            except Exception as e:
                logger.error(f"[ModelManager] OpenAI streaming error: {e}")
                return f"[OpenAI Async API Error] {str(e)}"

        else:
            return await asyncio.to_thread(
                self.generate, prompt, model_name=target_model, **kwargs
            )



"""
TokenizerManager - Manages tokenizers for local models.

Skips loading tokenizer if the model is an API-based model (e.g. OpenAI).
Caches tokenizers to avoid reloading.
"""

import logging
from transformers import AutoTokenizer
from utils.logging_utils import log_and_time
from models.model_manager import ModelManager

logger = logging.getLogger(__name__)
logger.debug("tokenizer_manager.py is alive")

class TokenizerManager:
    def __init__(self, model_manager=None):
        self.tokenizers = {}
        self.model_manager = model_manager

    @log_and_time("get tokenizer")
    def get_tokenizer(self, model_name):
        if model_name is None:
            raise ValueError("[TokenizerManager] No model_name provided.")

        # 🔥 Skip tokenizer loading for OpenAI models
        if self.model_manager and self.model_manager.is_api_model(model_name):
            logger.debug(f"[TokenizerManager] Skipping tokenizer load for API model '{model_name}'")
            return None

        # Patch for OpenAI-style names (fallback to gpt2 tokenizer)
        if model_name.lower() in {"gpt-3.5-turbo", "gpt-4", "gpt-4-turbo", "gpt-4.5-turbo"}:
            model_name = "gpt2"

        if model_name not in self.tokenizers:
            logger.debug(f"[TokenizerManager] Loading tokenizer for '{model_name}'")
            self.tokenizers[model_name] = AutoTokenizer.from_pretrained(model_name)

        return self.tokenizers[model_name]

# ✅ Manual test mode (run standalone)
if __name__ == "__main__":
    model_name = "mistralai/Mistral-7B-Instruct-v0.2"
    mm = ModelManager()
    mm.switch_model(model_name)
    logger.info(f"[ModelManager] Active model set to: {self.active_model_name}")

    tm = TokenizerManager(model_manager=mm)
    tokenizer = tm.get_tokenizer(model_name)
    sample = "Hello! This is a test message for token counting. How many tokens is it?"

    print(tokenizer.encode(sample))
    print("Token count:", len(tokenizer.encode(sample)))
import logging

# Use the root logger or create a child logger that will inherit handlers
logger = logging.getLogger(__name__)
logger.debug("Personality manager.py is alive")

import re
import json
import os
from datetime import datetime, timedelta
import os

class PersonalityManager:
    def __init__(self):
        self.personalities = {
            "default": {
                "system_prompt_file": "system_prompt_default.txt",
                "directives_file": "structured_directives.txt",
                "num_memories": 5,
                "include_wiki": True,
                "include_semantic_search": True,
                "include_summaries": True,
                "summary_limit": 5

            },
            "therapy": {
                "system_prompt_file": "system_prompt_therapy.txt",
                "directives_file": "structured_directives_therapy.txt",
                "num_memories": 30,
                "include_wiki": False,
                "include_semantic_search": False,
                "include_summaries": True,  # Important for therapy mode
                "summary_limit": 5

            },
            "snarky": {
                "system_prompt_file": "system_prompt_snarky.txt",
                "directives_file": "structured_directives_snarky.txt",
                "num_memories": 3,
                "include_wiki": True,
                "include_semantic_search": True,
                "include_summaries": True,  # Important for therapy mode
                "summary_limit": 5

            }
        }
        self.current_personality = "default"

    def switch_personality(self, name):
        if name in self.personalities:
            self.current_personality = name
            logger.debug(f"[PersonalityManager] Switched to: {name}")
        else:
            logger.debug(f"[PersonalityManager] Personality '{name}' not found. Using default.")

    def get_current_config(self):
        return self.personalities[self.current_personality]

# daemon_7_11_25_refactor/utils/file_processor.py
import docx2txt
import pandas as pd
from typing import List, Any
from utils.logging_utils import get_logger
logger = get_logger("file_processor")



class FileProcessor:
    """Handles processing of uploaded files"""

    def __init__(self):
        self.supported_extensions = ['.txt', '.docx', '.csv', '.py']

    async def process_files(self, user_text: str, files: List[Any]) -> str:
        """
        Process uploaded files and combine with user text

        Args:
            user_text: Original user input
            files: List of uploaded file objects

        Returns:
            Combined text including file contents
        """
        combined_text = user_text

        if not files:
            return combined_text

        logger.debug(f"Processing {len(files)} uploaded files")

        for file in files:
            try:
                file_content = self._process_single_file(file)
                combined_text += "\n\n" + file_content
                logger.debug(f"Successfully processed: {file.name}")

            except Exception as e:
                error_msg = f"[Error reading {file.name}: {str(e)}]"
                logger.error(f"File processing error: {e}")
                combined_text += "\n\n" + error_msg

        return combined_text

    def _process_single_file(self, file) -> str:
        """Process a single file based on its extension"""

        if file.name.endswith('.txt'):
            with open(file.name, 'r', encoding='utf-8') as f:
                return f.read()

        elif file.name.endswith('.docx'):
            return docx2txt.process(file.name)

        elif file.name.endswith('.csv'):
            df = pd.read_csv(file.name)
            return df.to_string()

        elif file.name.endswith('.py'):
            with open(file.name, 'r', encoding='utf-8') as f:
                return f"```python\n{f.read()}\n```"

        else:
            return f"[Unsupported file type: {file.name}]"

    def get_supported_extensions(self) -> List[str]:
        """Return list of supported file extensions"""
        return self.supported_extensions
import os
import sys
import time
import logging
import inspect
import threading
import functools
import multiprocessing
from datetime import datetime

# === Direct File Logger (Failsafe) ===
class DirectFileLogger:
    def __init__(self, filename="daemon_debug.log"):
        self.filename = filename
        self.lock = threading.Lock()

    def _write(self, level, logger_name, message):
        with self.lock:
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            line = f"{timestamp} [{level}] [{logger_name}] {message}\n"
            with open(self.filename, 'a', encoding='utf-8') as f:
                f.write(line)
                f.flush()
                os.fsync(f.fileno())

    def debug(self, msg, logger_name="daemon_app"):
        self._write("DEBUG", logger_name, msg)

    def info(self, msg, logger_name="daemon_app"):
        self._write("INFO", logger_name, msg)

    def warning(self, msg, logger_name="daemon_app"):
        self._write("WARNING", logger_name, msg)

    def error(self, msg, logger_name="daemon_app"):
        self._write("ERROR", logger_name, msg)

direct_logger = DirectFileLogger()

# === Logging Decorators ===
def log_and_time(label="Function"):
    def decorator(func):
        log = get_logger(func.__module__)

        if inspect.isasyncgenfunction(func):
            @functools.wraps(func)
            async def async_gen_wrapper(*args, **kwargs):
                start = time.time()
                log.debug(f"[{label}] START")
                async for result in func(*args, **kwargs):
                    yield result
                duration = time.time() - start
                log.debug(f"[{label}] END — Duration: {duration:.2f}s")
            return async_gen_wrapper

        elif inspect.iscoroutinefunction(func):
            @functools.wraps(func)
            async def async_func_wrapper(*args, **kwargs):
                start = time.time()
                log.debug(f"[{label}] START")
                result = await func(*args, **kwargs)
                duration = time.time() - start
                log.debug(f"[{label}] END — Duration: {duration:.2f}s")
                return result
            return async_func_wrapper

        elif inspect.isfunction(func):
            @functools.wraps(func)
            def sync_wrapper(*args, **kwargs):
                start = time.time()
                log.debug(f"[{label}] START")
                result = func(*args, **kwargs)
                duration = time.time() - start
                log.debug(f"[{label}] END — Duration: {duration:.2f}s")
                return result
            return sync_wrapper

        else:
            raise TypeError(f"@log_and_time cannot be applied to: {func.__name__}")
    return decorator


def log_duration(tag):
    def decorator(func):
        log = get_logger(func.__module__)

        if inspect.isasyncgenfunction(func):
            return func  # No good timing model for async generators
        if inspect.iscoroutinefunction(func):
            @functools.wraps(func)
            async def async_wrapper(*args, **kwargs):
                start = time.time()
                result = await func(*args, **kwargs)
                log.debug(f"[TIMING] {tag} took {time.time() - start:.2f}s")
                return result
            return async_wrapper
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            start = time.time()
            result = func(*args, **kwargs)
            log.debug(f"[TIMING] {tag} took {time.time() - start:.2f}s")
            return result
        return sync_wrapper
    return decorator


def with_logging(func):
    log = get_logger(func.__module__)

    if inspect.isasyncgenfunction(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            ensure_logging_persistence()
            async for item in func(*args, **kwargs):
                yield item
    else:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            ensure_logging_persistence()
            return await func(*args, **kwargs)
    return wrapper


def log_async_operation(func):
    log = get_logger(func.__module__)

    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        log.debug(f"[ASYNC START] {func.__name__}")
        try:
            result = await func(*args, **kwargs)
            log.debug(f"[ASYNC COMPLETE] {func.__name__}")
            return result
        except Exception as e:
            log.error(f"[ASYNC ERROR] {func.__name__}: {type(e).__name__}: {e}")
            raise
    return wrapper


# === Gradio Logger Wrapper ===

class GradioLogger:
    def __init__(self):
        self.direct_logger = direct_logger
        self.logger = logging.getLogger("daemon_app")

    def _log(self, level, msg):
        try:
            ensure_logging_persistence()
            getattr(self.logger, level)(msg)
        except Exception:
            pass
        getattr(self.direct_logger, level)(msg)

    def debug(self, msg): self._log("debug", msg)
    def info(self, msg): self._log("info", msg)
    def warning(self, msg): self._log("warning", msg)
    def error(self, msg): self._log("error", msg)

# === File Handler that Flushes Immediately ===

class ImmediateFileHandler(logging.FileHandler):
    def emit(self, record):
        super().emit(record)
        self.flush()
        direct_logger._write(record.levelname, record.name, record.getMessage())

# === Logging Setup ===

def setup_logging():
    import asyncio

    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    for name in list(logging.Logger.manager.loggerDict):
        logger_instance = logging.getLogger(name)
        logger_instance.handlers.clear()
        logger_instance.propagate = True
    root_logger.handlers.clear()

    formatter = logging.Formatter(
        "%(asctime)s [%(levelname)s] [%(name)s] %(message)s", "%Y-%m-%d %H:%M:%S"
    )

    file_handler = ImmediateFileHandler("daemon_debug.log", mode="w", encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    root_logger.addHandler(file_handler)

    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)

    logging.captureWarnings(True)
    multiprocessing.log_to_stderr().setLevel(logging.DEBUG)
    asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())

    critical_loggers = [
        '', '__main__', 'daemon_app', 'httpx', 'httpcore', 'openai', 'asyncio',
        'gradio', 'gradio.queueing', 'gradio.routes', 'transformers',
        'chromadb', 'models', 'hierarchical_memory', 'llm_gates', 'unified_hierarchical_prompt_builder'
    ]
    for name in critical_loggers:
        log = logging.getLogger(name)
        log.setLevel(logging.DEBUG)
        log.propagate = True

    root_logger.debug(f"🌀 New session started at {datetime.now().isoformat()}")
    direct_logger.debug(f"🌀 Direct logger also active at {datetime.now().isoformat()}")

    return root_logger

def ensure_logging_persistence():
    root_logger = logging.getLogger()
    if not any(isinstance(h, ImmediateFileHandler) for h in root_logger.handlers):
        setup_logging()
        direct_logger.warning("Logging was reset — reconfigured handlers")

def start_logging_monitor():
    def monitor():
        while True:
            time.sleep(5)
            ensure_logging_persistence()
            direct_logger.debug(f"Logging heartbeat — {datetime.now()}")
    threading.Thread(target=monitor, daemon=True).start()

def get_logger(name: str = "daemon_app") -> logging.Logger:
    ensure_logging_persistence()
    return logging.getLogger(name)

# === Initialize on Import ===

logger = setup_logging()
start_logging_monitor()
# time_manager.py
import json, os, logging
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class TimeManager:
    def __init__(self, time_file="data/last_query_time.json"):
        self.time_file = time_file
        self.last_query_time = self._load_last_query_time()
        self.last_response_time = None

    # ---------- persistence ----------
    def _load_last_query_time(self):
        if os.path.exists(self.time_file):
            try:
                with open(self.time_file, "r") as f:
                    return datetime.fromisoformat(json.load(f)["last_query_time"])
            except (json.JSONDecodeError, KeyError, ValueError):
                pass
        return None

    def _save_last_query_time(self):
        if self.last_query_time:
            with open(self.time_file, "w") as f:
                json.dump({"last_query_time": self.last_query_time.isoformat()}, f)

    # ---------- public helpers ----------
    def current(self) -> datetime:
        return datetime.now()

    def current_iso(self) -> str:
        return self.current().isoformat(sep=" ", timespec="seconds")

    def elapsed_since_last(self) -> str:
        if not self.last_query_time:
            return "N/A (first query)"
        delta = self.current() - self.last_query_time
        if delta.days:
            return f"{delta.days} d {delta.seconds//3600} h"
        if delta.seconds >= 3600:
            return f"{delta.seconds//3600} h {(delta.seconds%3600)//60} m"
        if delta.seconds >= 60:
            return f"{delta.seconds//60} m"
        return f"{delta.seconds} s"

    def mark_query_time(self) -> datetime:
        """Call at the *start* of request handling."""
        self.last_query_time = self.current()
        self._save_last_query_time()
        return self.last_query_time

    def measure_response(self, start_time, end_time):
        elapsed = end_time - start_time
        self.last_response_time = elapsed
        return f"{elapsed.total_seconds():.2f} s"

    def last_response(self) -> str:
        return f"{self.last_response_time.total_seconds():.2f} s" if self.last_response_time else "N/A"

import logging
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, CrossEncoder
import re
import json
import os
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from config.config import GATE_REL_THRESHOLD
from utils.logging_utils import log_and_time, get_logger

logger = get_logger(__name__)
logger.debug("llm gates.py is alive - now using cosine similarity")

@dataclass
class GateResult:
    relevant: bool
    confidence: float
    reasoning: str
    filtered_content: str = None

class CosineSimilarityGateSystem:
    """Fast cosine similarity-based gating system"""
    def __init__(self, model_manager, cosine_threshold=GATE_REL_THRESHOLD):
        self.model_manager = model_manager
        self.cosine_threshold = cosine_threshold
        self.cache = {}

        # Load embedding model for query encoding
        self.embed_model = SentenceTransformer("all-MiniLM-L6-v2")

        # Optional: Load cross-encoder for reranking (CPU-friendly)
        try:
            self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
            self.use_reranking = True
            logger.debug("[Cosine Gate] Cross-encoder loaded for reranking")
        except:
            self.cross_encoder = None
            self.use_reranking = False
            logger.debug("[Cosine Gate] No cross-encoder, using cosine only")

    def get_gating_model_name(self):
        return "cosine_similarity"

    def is_meta_query(self, query: str) -> bool:
        meta_keywords = ["memory", "memories", "what do you know", "how much do you remember",
                        "what have you stored", "what do you recall"]
        return any(kw in query.lower() for kw in meta_keywords)

    @log_and_time("Cosine Gate Content")
    async def gate_content_async(self, query: str, content: str, content_type: str) -> GateResult:
        """Gate content using cosine similarity"""
        cache_key = f"{query[:50]}:{content[:100]}:{content_type}"
        if cache_key in self.cache:
            logger.debug(f"[Cosine Gate] Cache hit for {content_type}")
            return self.cache[cache_key]

        try:
            # Encode query and content
            logger.debug(f"[Cosine Gate] Encoding query and {content_type} content")
            query_emb = self.embed_model.encode(query, convert_to_numpy=True)
            content_emb = self.embed_model.encode(content[:500], convert_to_numpy=True)

            # Calculate cosine similarity
            similarity = cosine_similarity([query_emb], [content_emb])[0][0]

            logger.debug(f"[Cosine Gate] {content_type} similarity: {similarity:.4f} (threshold: {self.cosine_threshold})")

            # Binary gating decision
            relevant = similarity >= self.cosine_threshold

            # Add keyword boosting for meta queries
            if self.is_meta_query(query) and any(word in content.lower() for word in ["memory", "stored", "recall"]):
                similarity += 0.1  # Boost score
                relevant = True
                logger.debug(f"[Cosine Gate] Meta query boost applied to {content_type}")

            result = GateResult(
                relevant=relevant,
                confidence=float(similarity),
                reasoning=f"Cosine similarity: {similarity:.3f}",
                filtered_content=content[:300] if relevant else None
            )

            self.cache[cache_key] = result
            logger.info(f"[Cosine Gate] {content_type}: sim={similarity:.3f}, relevant={relevant}, passed={'✓' if relevant else '✗'}")
            return result

        except Exception as e:
            logger.error(f"[Cosine Gate Error] {e}")
            return GateResult(relevant=True, confidence=0.5, reasoning="Gate failed, including by default")

class MultiStageGateSystem:
    def __init__(self, model_manager, cosine_threshold=GATE_REL_THRESHOLD):
        self.gate_system = CosineSimilarityGateSystem(model_manager, cosine_threshold)
        self.model_manager = model_manager
        self.embed_model = self.gate_system.embed_model

    def get_gating_model_name(self):
        return self.gate_system.get_gating_model_name()

    @log_and_time("Batch Cosine Gate Memories")
    async def batch_gate_memories(self, query: str, memories: List[Dict]) -> List[Dict]:
        """Batch gate memories using cosine similarity, bypassing episodic"""
        if not memories:
            return []

        try:
            episodic = []
            to_gate = []

            # Separate episodic from others
            for mem in memories:
                if mem.get("metadata", {}).get("type") == "episodic":
                    logger.debug(f"[Batch Gate] BYPASS for episodic memory: {mem.get('id', 'unknown')}")
                    episodic.append(mem)
                else:
                    to_gate.append(mem)

            logger.info(f"[Memory Filter] {len(episodic)} episodic memories bypassed gating")
            logger.info(f"[Memory Filter] Starting batch cosine gating for {len(to_gate)} memories")

            # Proceed with gating for the rest
            if not to_gate:
                return episodic  # nothing to gate

            query_emb = self.embed_model.encode(query, convert_to_numpy=True)
            contents = [mem.get('content', '')[:500] for mem in to_gate]
            memory_embs = self.embed_model.encode(contents, convert_to_numpy=True, batch_size=32)

            similarities = cosine_similarity([query_emb], memory_embs)[0]

            gated = []
            for mem, sim in zip(to_gate, similarities):
                if sim >= self.gate_system.cosine_threshold:
                    mem['relevance_score'] = float(sim)
                    mem['filtered_content'] = mem.get('content', '')[:300]
                    gated.append(mem)
                else:
                    logger.debug(f"[Batch Gate] Memory filtered out: score={sim:.3f}")

            # Reranking if needed
            if self.gate_system.use_reranking and len(gated) > 5:
                pairs = [[query, mem.get('content', '')[:300]] for mem in gated]
                rerank_scores = self.gate_system.cross_encoder.predict(pairs)
                for mem, score in zip(gated, rerank_scores):
                    mem['rerank_score'] = float(score)
                gated = sorted(gated, key=lambda x: x.get('rerank_score', 0), reverse=True)
            else:
                gated = sorted(gated, key=lambda x: x['relevance_score'], reverse=True)

            final = episodic + gated[:20 - len(episodic)]  # total cap of 20
            logger.info(f"[Memory Filter] Gating complete: {len(final)}/{len(memories)} memories passed")
            return final

        except Exception as e:
            logger.error(f"[Batch Gate Error] {e}")
            # Fallback return
            for i, mem in enumerate(memories[:10]):
                mem['relevance_score'] = 0.5 - (i * 0.05)
                mem['filtered_content'] = mem.get('content', '')[:300]
            return memories[:10]


    @log_and_time("Filter Memories")
    async def filter_memories(self, query: str, memories: List[Dict]) -> List[Dict]:
        """Main entry point - always use batch gating for efficiency"""
        if not memories:
            logger.debug("[Memory Filter] No memories to filter")
            return []

        logger.info(f"[Memory Filter] Starting batch cosine gating for {len(memories)} memories")
        filtered = await self.batch_gate_memories(query, memories)
        logger.info(f"[Memory Filter] Gating complete: {len(filtered)}/{len(memories)} memories passed")
        return filtered

    @log_and_time("Filter Wiki")
    async def filter_wiki_content(self, query: str, wiki_content: str) -> Tuple[bool, str]:
        if not wiki_content:
            return False, ""

        try:
            result = await self.gate_system.gate_content_async(query, wiki_content, "wikipedia")

            # ✅ Standard pass-through if relevant and above threshold
            if result.relevant and result.confidence > self.gate_system.cosine_threshold:
                return True, result.filtered_content or wiki_content[:500]

            # ✅ Secondary check: keyword fallback if cosine fails
            if not result.relevant and result.confidence < 0.25:
                query_keywords = set(re.findall(r'\b\w{4,}\b', query.lower()))
                wiki_keywords = set(re.findall(r'\b\w{4,}\b', wiki_content.lower()[:500]))
                overlap = query_keywords & wiki_keywords

                if len(overlap) >= 2:
                    logger.debug(f"[Wiki Filter Fallback] Cosine low ({result.confidence:.3f}), but keyword match passes: {overlap}")
                    return True, wiki_content[:500]

            # ✅ Backup fallback: slightly more lenient cosine pass
            if result.relevant and result.confidence > 0.4:
                return True, result.filtered_content or wiki_content[:500]

            return False, ""

        except Exception as e:
            logger.debug(f"[Wiki Filter Error] {e}")
            return False, ""


    @log_and_time("Filter Semantic")
    async def filter_semantic_chunks(self, query: str, chunks: List[Dict]) -> List[Dict]:
        if not chunks:
            logger.debug("[Semantic Filter] No chunks to filter")
            return []

        try:
            logger.info(f"[Semantic Filter] Processing {len(chunks)} semantic chunks")
            query_emb = self.embed_model.encode(query, convert_to_numpy=True)

            scored_chunks = []
            for i, chunk in enumerate(chunks[:30]):
                text = chunk.get('text', '')
                title = chunk.get('title', '')
                content = f"{title} {text[:300]}"
                chunk_emb = self.embed_model.encode(content, convert_to_numpy=True)

                cosine_score = cosine_similarity([query_emb], [chunk_emb])[0][0]
                chunk['relevance_score'] = float(cosine_score)
                chunk['filtered_content'] = text
                scored_chunks.append(chunk)

                if i < 5:  # Log first 5 for debugging
                    logger.debug(f"[Semantic Filter] Chunk {i} ({title}): score={cosine_score:.3f}")

            # Prefilter
            prefiltered = [c for c in scored_chunks if c['relevance_score'] >= 0.25]
            logger.info(f"[Semantic Filter] Pre-filter: {len(prefiltered)}/{len(scored_chunks)} chunks above 0.25 threshold")

            if self.gate_system.use_reranking and len(prefiltered) > 5:
                logger.info(f"[Semantic Filter] Running cross-encoder reranking on {len(prefiltered)} chunks")
                pairs = [[query, f"{c['title']} {c['text'][:300]}"] for c in prefiltered]
                rerank_scores = self.gate_system.cross_encoder.predict(pairs)

                for chunk, score in zip(prefiltered, rerank_scores):
                    chunk['rerank_score'] = float(score)

                sorted_chunks = sorted(prefiltered, key=lambda x: x['rerank_score'], reverse=True)
            else:
                sorted_chunks = sorted(prefiltered, key=lambda x: x['relevance_score'], reverse=True)

            top_chunks = sorted_chunks[:5]
            logger.info(f"[Semantic Filter] Final result: {len(top_chunks)} chunks selected")

            # Log the selected chunks
            for i, chunk in enumerate(top_chunks):
                logger.debug(f"[Semantic Filter] Selected chunk {i}: {chunk.get('title', 'untitled')} (score: {chunk.get('rerank_score', chunk.get('relevance_score')):.3f})")

            return top_chunks

        except Exception as e:
            logger.error(f"[Semantic Filter Error] {e}")
            import traceback
            logger.error(traceback.format_exc())
            return chunks[:3]



class GatedPromptBuilder:
    def __init__(self, prompt_builder, model_manager):
        self.prompt_builder = prompt_builder
        self.gate_system = MultiStageGateSystem(model_manager)

    @log_and_time("Cosine Gated Prompt Build")
    async def build_gated_prompt(self, user_input, memories, summaries, dreams,
                             wiki_snippet="", semantic_snippet=None,
                             semantic_memory_results=None, time_context=None,
                             recent_conversations=None, model_name=None,
                             include_dreams=True, include_code_snapshot=False,
                             include_changelog=False, system_prompt="",
                             directives_file="structured_directives.txt"):

        """Build prompt with cosine-similarity gated context."""
        logger.debug(f"[Gated Prompt] Building prompt with cosine similarity gating")

        filtered_context = {}

        # Gating each piece of context
        try:
            filtered_context["memories"] = await self.gate_system.filter_memories(user_input, memories)
        except Exception as e:
            logger.debug(f"[Gated Prompt - Memory Error] {e}")
            filtered_context["memories"] = memories[:5]

        try:
            include_wiki, filtered_wiki = await self.gate_system.filter_wiki_content(user_input, wiki_snippet)
            filtered_context["wiki_snippet"] = filtered_wiki if include_wiki else ""
        except Exception as e:
            logger.debug(f"[Gated Prompt - Wiki Error] {e}")
            filtered_context["wiki_snippet"] = ""

        try:
            filtered_context["semantic_chunks"] = await self.gate_system.filter_semantic_chunks(user_input, semantic_snippet or [])
        except Exception as e:
            logger.debug(f"[Gated Prompt - Semantic Error] {e}")
            filtered_context["semantic_chunks"] = []

        # Build prompt
        return self.prompt_builder.build_prompt(
            user_input=user_input,
            memories=filtered_context.get("memories", []),
            summaries=summaries,
            dreams=dreams if include_dreams else [],
            wiki_snippet=filtered_context.get("wiki_snippet", ""),
            semantic_snippet=filtered_context.get("semantic_chunks", []),
            semantic_memory_results=semantic_memory_results,
            time_context=time_context,
            model_name=model_name or self.prompt_builder.tokenizer_manager.active_model_name or
                        self.prompt_builder.model_manager.get_active_model_name(),
            is_api=True,
            include_dreams=include_dreams,
            include_code_snapshot=include_code_snapshot,
            include_changelog=include_changelog,
            system_prompt=system_prompt,
            directives_file=directives_file
        )

# daemon_7_11_25_refactor/daemon_7_11_25_refactor/knowledge/semantic_search.py
import os
import logging
import faiss
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from config.config import DEFAULT_TOP_K
from utils.logging_utils import log_and_time

logger = logging.getLogger(__name__)
logger.debug("knowledge.semantic_search is alive")

# === CONFIG (tweak these to your setup) ===
PARQUET_DIR       = "/run/media/lukeh/T9/test_parquet"
MERGED_PARQUET    = os.path.join(PARQUET_DIR, "merged_embeddings.parquet")
METADATA_FILE     = "metadata.parquet"
FAISS_INDEX_FILE  = "vector_index_ivf.faiss"
MODEL_NAME        = "all-MiniLM-L6-v2"
TOP_K             = DEFAULT_TOP_K

# — load or build your index & metadata once on import —
@log_and_time("Load embeddings + metadata")
def _load_resources():
    global model, index, metadata_df

    # 1) load embedding model
    model = SentenceTransformer(MODEL_NAME)

    # 2) load FAISS index
    try:
        if os.path.exists(FAISS_INDEX_FILE):
            index = faiss.read_index(FAISS_INDEX_FILE)
        else:
            logger.warning(f"FAISS index not found: {FAISS_INDEX_FILE}. Semantic search will be disabled.")
            index = None
    except Exception as e:
        logger.error(f"Failed to load FAISS index: {e}")
        index = None

    # 3) load metadata parquet
    if os.path.exists(METADATA_FILE):
        metadata_df = pd.read_parquet(METADATA_FILE)
    else:
        raise FileNotFoundError(f"Metadata file not found: {METADATA_FILE}")

_load_resources()

@log_and_time("Semantic search")
def semantic_search(query: str, top_k: int = TOP_K) -> list:
    """
    Perform a FAISS-based semantic search over your pre-loaded index.
    Returns a list of dicts with keys: rank, score, similarity, id, title, text.
    """
    # encode query
    q_emb = model.encode([query], convert_to_numpy=True).astype("float32")
    # search
    D, I = index.search(q_emb, top_k)

    results = []
    for rank, (dist, idx) in enumerate(zip(D[0], I[0]), start=1):
        if idx < 0:
            continue
        row = metadata_df.iloc[idx]
        results.append({
            "rank": rank,
            "score": float(dist),
            "similarity": 1.0 / (1.0 + dist),
            "id":    row["id"],
            "title": row["title"],
            "text":  row["text"][:500]
        })
    return results
# semantic_chunker.py
import logging

# Use the root logger or create a child logger that will inherit handlers
logger = logging.getLogger(__name__)
logger.debug("topic_manager.py is alive")
import json
from collections import Counter
import spacy
import os
from utils.logging_utils import log_and_time

class TopicManager:
    def __init__(self, top_topics_file="data/top_topics.json"):
        self.top_topics_file = top_topics_file
        self.default_common_topics = [
            "World War II", "United States", "Artificial Intelligence",
            "Climate Change", "Photosynthesis", "Quantum Mechanics",
            "Neural Networks", "Moon Landing", "French Revolution",
        ]
        self.user_topic_counter = Counter()
        self._load_top_topics()
        self.nlp = spacy.load("en_core_web_sm")

    def _load_top_topics(self):
        if os.path.exists(self.top_topics_file):
            with open(self.top_topics_file, "r") as f:
                self.top_topics = set(json.load(f))
        else:
            self.top_topics = set(self.default_common_topics)

    def extract_nouns(self, text):
        doc = self.nlp(text)
        return [token.text.lower() for token in doc if token.pos_ == "NOUN"]

    def extract_entities_and_topics(self, text):
        """Extract named entities and important nouns from text"""
        doc = self.nlp(text)

        # Get named entities (proper nouns, places, etc.)
        entities = [ent.text for ent in doc.ents]

        # Get important nouns (not just any noun)
        important_nouns = []
        for token in doc:
            if (token.pos_ == "NOUN" and
                len(token.text) > 3 and  # Skip short words
                not token.is_stop and    # Skip stop words
                token.is_alpha):         # Skip numbers/punctuation
                important_nouns.append(token.text.title())  # Capitalize

        # Combine and deduplicate
        topics = list(set(entities + important_nouns))
        return topics[:10]  # Return top 10 most relevant

    def update_from_user_input(self, text):
        # Extract topics from current input
        current_topics = self.extract_entities_and_topics(text)

        # Update the counter for long-term tracking
        nouns = self.extract_nouns(text)
        self.user_topic_counter.update(nouns)

        # Use current topics as the active topics for this query
        self.top_topics = set(current_topics)

        return current_topics

    def refresh_top_topics(self):
        popular_user_topics = [topic for topic, count in self.user_topic_counter.items() if count >= 3]
        combined = set(self.default_common_topics).union(popular_user_topics)
        with open(self.top_topics_file, "w") as f:
            json.dump(list(combined), f, indent=2)
        print("[TopicManager] Refreshed top topics.")
# WikiManager.py

"""
WikiManager - Retrieves Wikipedia summaries and full articles.

Responsibilities:
- Search and return article summaries
- Optionally return full article content
- Heuristic for fallback when summary is insufficient
"""
import logging

# Use the root logger or create a child logger that will inherit handlers
logger = logging.getLogger(__name__)
logger.debug("Wiki Manager.py is alive")

import re
import json
import os
from datetime import datetime, timedelta
import wikipedia
from utils.logging_utils import log_and_time

class WikiManager:
    def __init__(self, offline_mode=False):
        self.offline_mode = offline_mode
        # Future: load local index if offline_mode is True
    @log_and_time("Search summary")
    def search_summary(self, topic, sentences=3):
        """Fetch a summary for the given topic."""
        try:
            # Capitalize first letter as wikipedia package is case-sensitive
            topic = topic.strip().capitalize()

            summary = wikipedia.summary(topic, sentences=sentences, auto_suggest=True, redirect=True)
            return summary
        except wikipedia.exceptions.DisambiguationError as e:
            logger.warning(f"[WikiManager] Disambiguation error for '{topic}': {e.options}")
            return f"[Disambiguation Error] Topic '{topic}' is ambiguous. Options: {e.options}"
        except wikipedia.exceptions.PageError:
            logger.warning(f"[WikiManager] Page error: '{topic}' not found.")
            return f"[Page Error] Topic '{topic}' not found."
        except Exception as e:
            logger.error(f"[WikiManager] Unexpected error for '{topic}': {str(e)}")
            return f"[Error fetching summary: {str(e)}]"
    @log_and_time("Fetch full article")
    def fetch_full_article(self, topic):
        """Fetch the full article content for the given topic."""
        try:
            page = wikipedia.page(topic, auto_suggest=True, redirect=True)
            return page.content
        except wikipedia.exceptions.DisambiguationError as e:
            return f"[Disambiguation Error] Topic '{topic}' is ambiguous. Options: {e.options}"
        except wikipedia.exceptions.PageError:
            return f"[Page Error] Topic '{topic}' not found."
        except Exception as e:
            return f"[Error fetching article: {str(e)}]"

    def should_fallback(self, summary, query):
        """Decide whether full article exploration might be needed."""
        if not summary or len(summary.split()) < 50:
            return True
        # If asking for numeric or structured data
        keywords = ["table", "statistics", "list", "timeline", "formula", "equation"]
        if any(k in query.lower() for k in keywords):
            return True
        return False
# embed_wiki_chunks_to_parquet.py
print("UPGRADED PARQUET EMBEDDER LAUNCHED", flush=True)
# === API Keys and Config ===
import os
import logging

# Use the root logger or create a child logger that will inherit handlers
logger = logging.getLogger(__name__)
logger.debug("embed_wiki_chunks_to_parquet.py is alive")
import os
import json
import sys
import shutil
import time
import queue
import threading
from pathlib import Path
from datetime import datetime
import pandas as pd
import torch
from sentence_transformers import SentenceTransformer
from more_itertools import chunked
import logging
# from Injection_Protection_Feed_to_Embeder import sanitize_text # Commented out if not available

# ==== TEST MODE CONFIG ====
TEST_MODE = os.environ.get("TEST_MODE") == "1" # Ensure TEST_MODE is read directly here

# ==== CONFIGURATION ====
CHUNK_SIZE = 512 # These seem to be internal to semantic_chunk_text, not used externally for producer
OVERLAP = 256
ENCODE_BATCH_SIZE = 8
SAFE_BATCH_SIZE = 50
HEARTBEAT_INTERVAL = 100
MIN_FREE_GB = 10
MAX_QUEUE_SIZE = 100
NUM_CONSUMER_THREADS = int(os.environ.get("NUM_CONSUMER_THREADS", "1"))

# WIKI_PATH refers to the input directory for basic chunks.
# We need to adjust this to point to semantic_chunks if using semantic chunking.
# However, the producer function will handle globbing, so it's less critical as a global path.
# For clarity, let's make the producer aware of where to look.
# WIKI_PATH = Path("/run/media/lukeh/T9/quarentine_chunks") # Original, potentially unused or misnamed
SEMANTIC_CHUNKS_SOURCE_DIR = Path("semantic_chunks") # New variable to point to semantic chunks
BASIC_CHUNKS_SOURCE_DIR = Path("wiki_chunks") # New variable to point to basic chunks

PARQUET_PATH = Path("embedded_parquet")
CHECKPOINT_FILE = Path("embed_checkpoint.json")
PARQUET_PATH.mkdir(parents=True, exist_ok=True)

# ==== DISK CHECK ====
def check_disk_space(path="/"):
    total, used, free = shutil.disk_usage(path)
    free_gb = free // (2**30)
    if free_gb < MIN_FREE_GB:
        print(f"🛑 LOW DISK SPACE: Only {free_gb} GB free. Exiting.")
        sys.exit(1)
    print(f"[INFO] Disk check passed: {free_gb} GB free")

check_disk_space("/run/media/lukeh/T9")

# ==== INIT ====
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"[INFO] Using device: {device}", flush=True)
model = SentenceTransformer("all-MiniLM-L6-v2", device=device)
model.max_seq_length = 384

# ==== CHECKPOINT ====
def load_checkpoint():
    if CHECKPOINT_FILE.exists():
        with open(CHECKPOINT_FILE, "r") as f:
            return set(json.load(f))
    return set()

def save_checkpoint(done_chunks):
    with open(CHECKPOINT_FILE, "w") as f:
        json.dump(sorted(list(done_chunks)), f)

done_chunks = load_checkpoint()
logger.debug(f" Total files already done: {len(done_chunks)}")

# ==== CHUNKING ====
# This semantic_chunk_text function is redundant if semantic_chunker.py is used.
# The embedder should *not* be re-chunking. It should consume already chunked data.
# Let's assume it's kept for the 'basic' pipeline path for now, but for semantic, it's skipped.
def semantic_chunk_text(text, target_size=200, max_size=250, min_size=90):
    import re
    paragraphs = text.split('\n\n')
    chunks = []
    current_chunk = ""

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        if len(para) > max_size:
            sentences = re.split(r'(?<=[.!?])\s+', para)
            for sentence in sentences:
                if len(current_chunk) + len(sentence) + 1 < target_size:
                    current_chunk += sentence + " "
                else:
                    if len(current_chunk) >= min_size:
                        chunks.append(current_chunk.strip())
                        current_chunk = sentence + " "
                    else:
                        current_chunk += sentence + " "
        else:
            if len(current_chunk) + len(para) + 2 < target_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"

    if current_chunk and len(current_chunk) >= min_size:
        chunks.append(current_chunk.strip())

    return chunks

def chunk_text_with_metadata(text, source_name):
    # This function is also for basic chunking.
    # When semantic chunking is used, this function will not be called for the embedder.
    chunks_data = []
    chunk_texts = semantic_chunk_text(text) # Uses the internal semantic_chunk_text
    current_pos = 0

    for i, chunk_text in enumerate(chunk_texts):
        char_start = text.find(chunk_text, current_pos)
        if char_start == -1:
            char_start = current_pos
        char_end = char_start + len(chunk_text)
        current_pos = char_end

        chunks_data.append({
            'text': chunk_text,
            'chunk_index': i,
            'total_chunks': len(chunk_texts),
            'prev_chunk': chunk_texts[i-1][:100] if i > 0 else "",
            'next_chunk': chunk_texts[i+1][:100] if i < len(chunk_texts)-1 else "",
            'char_start': char_start,
            'char_end': char_end
        })

    return chunks_data

# ==== EMBEDDER ====
def embedder(file_queue, done_chunks):
    processed = 0
    total_time = 0
    total_chunks = 0
    done_articles_count = len(done_chunks)

    while True:
        item = file_queue.get()
        if item is None:
            break

        source_name, title, chunks_data = item

        if source_name in done_chunks:
            print(f"[SKIP EMBEDDER] {source_name} already done.", flush=True)
            file_queue.task_done()
            continue

        logger.debug(f" Embedding chunk for {title} (source: {source_name})...", flush=True)

        start_time = time.time()
        try:
            texts = [c['text'][:model.max_seq_length] for c in chunks_data]
            texts = [t.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore") for t in texts]

            embeddings = model.encode(
                texts,
                batch_size=ENCODE_BATCH_SIZE,
                show_progress_bar=False,
                device=device,
                normalize_embeddings=True
            )

            # Call the REVISED store_to_parquet here
            store_to_parquet(source_name, title, chunks_data, embeddings)
            done_chunks.add(source_name)

            if device == "cuda":
                torch.cuda.empty_cache()

            end_time = time.time()
            elapsed = end_time - start_time
            # Correct output_parquet_path for console message
            stem = source_name.replace(" ", "_").replace("/", "_").replace(":", "_").replace("__", "_")
            output_parquet_path = PARQUET_PATH / f"{stem}.parquet"
            file_size = os.path.getsize(output_parquet_path) / (1024 * 1024)

            print(f"✅ Finished {source_name}: {len(chunks_data)} chunks, {file_size:.2f} MB, {elapsed:.2f} sec", flush=True)

            total_time += elapsed
            total_chunks += len(chunks_data)
            processed += 1

            if processed % HEARTBEAT_INTERVAL == 0:
                save_checkpoint(done_chunks)

        except Exception as e:
            print(f"[ERROR] Failed to embed {source_name}: {e}", flush=True)
            import traceback
            traceback.print_exc()
            file_queue.task_done()
            continue
        file_queue.task_done()

    save_checkpoint(done_chunks)
    print(f"🎉 All done: {processed} files (articles), {total_chunks} total chunks, {total_time:.2f} sec total", flush=True)
    print(f"🧠 Average time per file: {total_time / max(1, processed):.2f} sec")
    print(f"📊 Average time per chunk: {total_time / max(1, total_chunks):.2f} sec")


# REVISED PRODUCER TO GROUP CHUNKS BY ARTICLE (for semantic chunks)
def producer(file_queue, done_chunks):
    import glob
    jsonl_files = sorted(glob.glob(str(SEMANTIC_CHUNKS_SOURCE_DIR / "*.jsonl")))

    article_chunks_buffer = {}
    test_article_count = 0

    for jsonl_path in jsonl_files:
        if TEST_MODE and test_article_count >= 10:
            print(f"🧪 Test mode: queued {test_article_count} articles (or parts) — exiting producer loop.", flush=True)
            break

        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                data = json.loads(line)
                chunk_text = data['text']
                metadata = data['metadata']

                article_title = metadata.get('title', 'Unknown Title')
                page_id = metadata.get('page_id', 'Unknown Page ID')
                article_key = (article_title, page_id)
                article_identifier = f"{article_title}_{page_id}"

                if article_identifier in done_chunks:
                    continue

                if TEST_MODE and test_article_count >= 10 and article_key not in article_chunks_buffer:
                    break

                if article_key not in article_chunks_buffer:
                    article_chunks_buffer[article_key] = []
                    if TEST_MODE:
                        test_article_count += 1

                article_chunks_buffer[article_key].append({
                    'text': chunk_text,
                    'chunk_index': metadata.get('chunk_idx', 0),
                    'total_chunks': metadata.get('total_chunks', 1),
                    'prev_snippet': '',
                    'next_snippet': '',
                    'char_start': 0,
                    'char_end': len(chunk_text),
                    'section': metadata.get('section', ''),
                    'section_level': metadata.get('section_level', 0)
                })

        for article_key, chunks_list in list(article_chunks_buffer.items()):
            article_title, page_id = article_key
            article_identifier = f"{article_title}_{page_id}"

            if article_identifier in done_chunks or (TEST_MODE and test_article_count >= 10):
                pass

            print(f"[QUEUEING] Article {article_identifier} with {len(chunks_list)} chunks", flush=True)
            file_queue.put((article_identifier, article_title, chunks_list))
            del article_chunks_buffer[article_key]

    for article_key, chunks_list in article_chunks_buffer.items():
        article_title, page_id = article_key
        article_identifier = f"{article_title}_{page_id}"
        if article_identifier in done_chunks:
            continue
        print(f"[QUEUEING] Remaining Article {article_identifier} with {len(chunks_list)} chunks", flush=True)
        file_queue.put((article_identifier, article_title, chunks_list))

    for _ in range(NUM_CONSUMER_THREADS):
        file_queue.put(None)


# REVISED store_to_parquet to handle source_name as article identifier
def store_to_parquet(source_name, title, chunks_data, embeddings):
    safe_filename = source_name.replace(" ", "_").replace("/", "_").replace(":", "_").replace("__", "_")
    output_path = PARQUET_PATH / f"{safe_filename}.parquet"

    df = pd.DataFrame({
        "id": [f"{title}_{c.get('chunk_index', i)}" for i, c in enumerate(chunks_data)],
        "title": [title] * len(chunks_data),
        "text": [c['text'] for c in chunks_data],
        "embedding": [emb.tolist() for emb in embeddings],
        "chunk_index": [c.get('chunk_index', i) for i, c in enumerate(chunks_data)],
        "total_chunks": [c.get('total_chunks', len(chunks_data)) for c in chunks_data],
        "prev_snippet": [c.get('prev_snippet', '') for c in chunks_data],
        "next_snippet": [c.get('next_snippet', '') for c in chunks_data],
        "char_start": [c.get('char_start', 0) for c in chunks_data],
        "char_end": [c.get('char_end', len(c['text'])) for c in chunks_data],
        "section": [c.get('section', '') for c in chunks_data],
        "section_level": [c.get('section_level', 0) for c in chunks_data]
    })
    df.to_parquet(output_path, index=False)

# ==== MAIN ====
def run_embedding_pipeline():
    print("✅ Starting upgraded embedding pipeline", flush=True)
    file_queue = queue.Queue(MAX_QUEUE_SIZE)

    prod_thread = threading.Thread(target=producer, args=(file_queue, done_chunks))
    prod_thread.start()

    consumer_threads = []
    for _ in range(NUM_CONSUMER_THREADS):
        cons_thread = threading.Thread(target=embedder, args=(file_queue, done_chunks))
        cons_thread.start()
        consumer_threads.append(cons_thread)

    prod_thread.join()
    for cons_thread in consumer_threads:
        cons_thread.join()

    print("🎉 Embedding pipeline finished.", flush=True)
import os
import re
from concurrent.futures import ThreadPoolExecutor
from lxml import etree
# === API Keys and Config ===
import os
import logging

# Use the root logger or create a child logger that will inherit handlers
logger = logging.getLogger(__name__)
logger.debug("extract_wikipedia_articles.py is alive")

# === CONFIG ===
BASE_DIR = os.path.dirname(__file__)
XML_FILE = os.path.join(BASE_DIR, "wiki_data", "enwiki-latest-pages-articles.xml")
OUTPUT_DIR = os.path.join(BASE_DIR, "wiki_articles")
MAX_WORKERS = 8  # Adjust depending on CPU

# === FUNCTIONS ===

def clean_text(text):
    """Basic cleanup for article text."""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def save_article(title, text):
    """Save a single article to disk."""
    safe_title = re.sub(r'[^a-zA-Z0-9_\-]', '_', title)
    filename = f"{safe_title[:100]}.txt"
    filepath = os.path.join(OUTPUT_DIR, filename)

    with open(filepath, "w", encoding="utf-8") as f_out:
        f_out.write(clean_text(text))

def process_wiki_dump(xml_file):
    """Extract and save articles from the Wikipedia dump."""
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    context = etree.iterparse(xml_file, events=("end",), tag="{*}page")
    article_count = 0

    print("[INFO] Extraction started...")
    first_titles = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = []

        for event, elem in context:
            title_elem = elem.find("./{*}title")
            text_elem = elem.find(".//{*}text")

            if title_elem is not None and text_elem is not None:
                title = title_elem.text or "Untitled"
                text = text_elem.text or ""

                if len(text.strip()) > 100:
                    futures.append(executor.submit(save_article, title, text))
                    article_count += 1

                    if len(first_titles) < 3:
                        first_titles.append(title)

                    if article_count % 100 == 0:
                        print(f"[PROGRESS] {article_count} articles scheduled...")

            elem.clear()

    for future in futures:
        future.result()

    print(f"[DONE] Extraction complete. {article_count} articles saved.")
    logger.debug(f" First few titles extracted: {first_titles}")
def stream_extract_articles(xml_file):
    """Generator to stream Wikipedia articles as XML elements."""
    context = etree.iterparse(xml_file, events=("end",), tag="{*}page")

    for event, elem in context:
        title_elem = elem.find("./{*}title")
        text_elem = elem.find(".//{*}text")

        if title_elem is not None and text_elem is not None:
            title = title_elem.text or "Untitled"
            text = text_elem.text or ""

            yield {
                "title": title,
                "text": text
            }

        elem.clear()

# === MAIN ===

if __name__ == "__main__":
    print("[INIT] Starting Wikipedia extraction...")
    process_wiki_dump(XML_FILE)
import re
import logging

# Use the root logger or create a child logger that will inherit handlers
logger = logging.getLogger(__name__)
logger.debug("injection_protection_feed_to_embeder.py is alive")

# Define suspicious patterns
multilingual_patterns = {
    'english': [r'ignore previous instructions', r'instead respond with'],
    'chinese': [r'忽略之前的指令', r'改为以下回复'],
    'spanish': [r'ignora las instrucciones anteriores', r'en su lugar responde con'],
    'russian': [r'игнорировать предыдущие инструкции', r'вместо этого ответить'],
    'arabic': [r'تجاهل التعليمات السابقة', r'بدلاً من ذلك رد بـ'],
    'french': [r'ignorez les instructions précédentes', r'répondez plutôt par'],
    'german': [r'ignoriere vorherige anweisungen', r'antworte stattdessen mit']
}

def sanitize_text(text, chunk_id=None, log=True):
    flagged = False
    flagged_phrases = []
    for lang, patterns in multilingual_patterns.items():
        for pattern in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                flagged = True
                flagged_phrases.append((lang, pattern))
                text = re.sub(pattern, '[REDACTED SUSPICIOUS PHRASE]', text, flags=re.IGNORECASE)

    if flagged and log:
        logging.info(f"FLAGGED CHUNK: {chunk_id} | PHRASES: {flagged_phrases}")

    return text
# semantic_chunker.py
import logging

# Use the root logger or create a child logger that will inherit handlers
logger = logging.getLogger(__name__)
logger.debug("semantic_chunker.py is alive")
import os
import re
import json
from typing import List, Dict
from extract_wikipedia_articles import stream_extract_articles

class SemanticWikiChunker:
    """
    Chunks Wikipedia articles semantically for optimal FAISS embedding.
    """

    def __init__(self,
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200,
                 min_chunk_size: int = 100):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size

    def parse_wiki_sections(self, wiki_text: str) -> List[Dict]:
        sections = []
        current_section = {"title": "Introduction", "content": [], "level": 0}

        for line in wiki_text.split('\n'):
            header_match = re.match(r'^(=+)\s*(.*?)\s*\1$', line)
            if header_match:
                if current_section['content']:
                    current_section['content'] = '\n'.join(current_section['content'])
                    sections.append(current_section)

                level = len(header_match.group(1))
                current_section = {
                    "title": header_match.group(2),
                    "content": [],
                    "level": level
                }
            else:
                if line.strip():
                    current_section['content'].append(line)

        if current_section['content']:
            current_section['content'] = '\n'.join(current_section['content'])
            sections.append(current_section)

        return sections

    def clean_wiki_markup(self, text: str) -> str:
        text = re.sub(r'\{\{[^}]+\}\}', '', text)
        text = re.sub(r'<ref[^>]*>.*?</ref>', '', text)
        text = re.sub(r'\[\[(?:[^|\]]+\|)?([^\]]+)\]\]', r'\1', text)
        text = re.sub(r'\[https?://[^\s\]]+\s*([^\]]*)\]', r'\1', text)
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def estimate_tokens(self, text: str) -> int:
        return len(text) // 4

    def chunk_text(self, text: str, metadata: Dict) -> List[Dict]:
        words = text.split()
        chunks = []
        if not words:
            return chunks

        words_per_chunk = int(self.chunk_size / 1.5)
        words_per_overlap = int(self.chunk_overlap / 1.5)

        start = 0
        chunk_idx = 0

        while start < len(words):
            end = start + words_per_chunk
            chunk_text = ' '.join(words[start:end])

            if self.estimate_tokens(chunk_text) >= self.min_chunk_size:
                chunks.append({
                    'text': chunk_text,
                    'metadata': {
                        **metadata,
                        'chunk_idx': chunk_idx,
                        'chunk_start': start,
                        'chunk_end': min(end, len(words))
                    }
                })
                chunk_idx += 1

            start = end - words_per_overlap
            if start >= len(words):
                break

        return chunks

    def process_article(self, article_dict: Dict) -> List[Dict]:
        # Use article_dict from stream_extract_articles
        article_data = {
            'title': article_dict.get('title', 'Unknown'),
            'page_id': article_dict.get('page_id', 'Unknown'),  # Optional, if you add page_id
            'raw_text': article_dict.get('text', '')
        }

        if not article_data['raw_text']:
            return []

        sections = self.parse_wiki_sections(article_data['raw_text'])
        all_chunks = []

        for section in sections:
            clean_text = self.clean_wiki_markup(section['content'])
            if not clean_text:
                continue

            section_metadata = {
                'title': article_data['title'],
                'page_id': article_data['page_id'],
                'section': section['title'],
                'section_level': section['level']
            }

            if self.estimate_tokens(clean_text) > self.chunk_size:
                chunks = self.chunk_text(clean_text, section_metadata)
                all_chunks.extend(chunks)
            else:
                all_chunks.append({
                    'text': clean_text,
                    'metadata': {
                        **section_metadata,
                        'chunk_idx': 0,
                        'chunk_start': 0,
                        'chunk_end': len(clean_text.split())
                    }
                })

        return all_chunks

    def save_chunks_jsonl(self, chunks: List[Dict], output_path: str):
        with open(output_path, 'w', encoding='utf-8') as f:
            for chunk in chunks:
                f.write(json.dumps(chunk, ensure_ascii=False) + '\n')

    def stream_process_wikipedia(self, xml_path: str, output_dir: str,
                                 batch_size: int = 1000, max_articles: int = None):
        os.makedirs(output_dir, exist_ok=True)

        chunk_buffer = []
        file_idx = 0
        total_articles = 0
        total_chunks = 0

        for article_dict in stream_extract_articles(xml_path):
            if max_articles and total_articles >= max_articles:
                break

            chunks = self.process_article(article_dict)
            chunk_buffer.extend(chunks)
            total_articles += 1
            total_chunks += len(chunks)

            if total_articles % 10 == 0:
                logger.debug(f"[PROGRESS] {total_articles} articles → {total_chunks} chunks")

            if len(chunk_buffer) >= batch_size:
                output_path = os.path.join(output_dir, f'semantic_chunks_{file_idx:04}.jsonl')
                self.save_chunks_jsonl(chunk_buffer, output_path)
                logger.debug(f"[SAVED] {len(chunk_buffer)} chunks to {output_path}")
                chunk_buffer = []
                file_idx += 1

        if chunk_buffer:
            output_path = os.path.join(output_dir, f'semantic_chunks_{file_idx:04}.jsonl')
            self.save_chunks_jsonl(chunk_buffer, output_path)
            logger.debug(f"[SAVED] Final {len(chunk_buffer)} chunks")

        logger.debug(f"[COMPLETE] {total_articles} articles → {total_chunks} semantic chunks")
# unified_pipeline.py
# semantic_chunker.py
import logging

# Use the root logger or create a child logger that will inherit handlers
logger = logging.getLogger(__name__)
logger.debug("unified_pipeline.py is alive")
import argparse
import os
import subprocess
import urllib.request
from pathlib import Path
import sys
import json
from typing import Dict, List
import logging

# Import both pipelines
# Make sure these are callable as functions
from embed_wiki_chunks_to_parquet import run_embedding_pipeline
from semantic_chunker import SemanticWikiChunker

# Default paths
DEFAULT_WIKI_URL = "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"
WIKI_ZIP_PATH = Path("wiki_data/enwiki-latest-pages-articles.xml.bz2")
EXTRACTED_PATH = Path("wiki_data/enwiki-latest-pages-articles.xml")
CHUNKS_DIR = Path("wiki_chunks") # This is for basic chunking
SEMANTIC_CHUNKS_DIR = Path("semantic_chunks") # This is where semantic chunker outputs

# Define TEST_MODE globally or pass it
TEST_MODE = os.environ.get("TEST_MODE") == "1" # Add this line

def download_wikipedia_dump(url, output_path):
    """Download Wikipedia dump from URL with progress reporting."""
    logger.debug(f"🌐 Downloading Wikipedia dump from: {url}")
    logger.debug(f"📁 Saving to: {output_path}")

    # Ensure directory exists
    output_path.parent.mkdir(parents=True, exist_ok=True)

    def download_progress(block_num, block_size, total_size):
        downloaded = block_num * block_size
        percent = min(downloaded * 100 / total_size, 100)
        mb_downloaded = downloaded / 1024 / 1024
        mb_total = total_size / 1024 / 1024
        sys.stdout.write(f'\r⬇️  Progress: {percent:.1f}% ({mb_downloaded:.1f}/{mb_total:.1f} MB)')
        sys.stdout.flush()

    try:
        urllib.request.urlretrieve(url, output_path, reporthook=download_progress)
        print("\n✅ Download complete!")
        return True
    except Exception as e:
        print(f"\n❌ Download failed: {e}")
        return False

def unzip_wikipedia_dump(zip_path):
    """Unzip bz2 compressed Wikipedia dump."""
    print("📦 Unzipping Wikipedia dump...", flush=True)

    if not zip_path.exists():
        print(f"❌ File not found: {zip_path}")
        return False

    # Use -k to keep the original file, -f to force overwrite if exists
    result = subprocess.run(
        ["bzip2", "-dkf", str(zip_path)],
        capture_output=True,
        text=True
    )

    if result.returncode == 0:
        print("✅ Unzipping complete.")
        return True
    else:
        print("❌ Unzip failed:", result.stderr)
        return False

def run_basic_extractor():
    """Run the original article extraction (500-article chunks)."""
    print("🔍 Running BASIC article extraction (500-article chunks)...", flush=True)

    if not EXTRACTED_PATH.exists():
        print(f"❌ Extracted XML not found at {EXTRACTED_PATH}")
        return False

    result = subprocess.run(
        ["python", "extract_wikipedia_articles.py"], # Assumes extract_wikipedia_articles.py exists
        capture_output=True,
        text=True
    )

    print(result.stdout)
    if result.stderr:
        print("[Extractor STDERR]", result.stderr)

    return result.returncode == 0

def run_semantic_extractor(xml_path=None, chunk_size=1000, chunk_overlap=200, min_chunk_size=100, max_articles=None):
    global TEST_MODE
    if TEST_MODE:
        print("🛑 TEST_MODE is on — skipping semantic extractor!", flush=True)
        return True

    print("🧩 Running SEMANTIC extraction and chunking...", flush=True)

    xml_path = xml_path or EXTRACTED_PATH
    if not xml_path.exists():
        print(f"❌ Extracted XML not found at {xml_path}")
        return False

    try:
        chunker = SemanticWikiChunker(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            min_chunk_size=min_chunk_size
        )

        chunker.stream_process_wikipedia(
            str(xml_path),
            str(SEMANTIC_CHUNKS_DIR),
            batch_size=1000,
            max_articles=max_articles  # <<< Pass it here!
        )

        return True
    except Exception as e:
        print(f"❌ Semantic chunking failed: {e}")
        import traceback
        traceback.print_exc()
        return False

# Remove convert_jsonl_to_text_format and embed_semantic_chunks
# We will directly use embed_wiki_chunks_to_parquet with the JSONL files.

def main():
    parser = argparse.ArgumentParser(
        description="Daemon Wikipedia Embedding Pipeline Orchestrator",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python pipeline.py --download --semantic
  python pipeline.py --download
  python pipeline.py --compressed wiki_data/enwiki.xml.bz2 --semantic
  python pipeline.py --extracted wiki_data/enwiki.xml --semantic
  python pipeline.py --extracted data.xml --test --semantic
        """
    )

    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("--download", action="store_true", help="Download Wikipedia dump")
    input_group.add_argument("--compressed", type=Path, metavar="PATH", help="Use existing .bz2 file")
    input_group.add_argument("--extracted", type=Path, metavar="PATH", help="Use extracted XML")

    parser.add_argument("--semantic", action="store_true", help="Use semantic chunking")
    parser.add_argument("--url", type=str, default=DEFAULT_WIKI_URL, help="Download URL")
    parser.add_argument("--source", type=str, default="wikipedia.com", help="Source label (currently not directly used in embedding)")
    parser.add_argument("--test", action="store_true", help="Run in test mode")
    parser.add_argument("--skip-extraction", action="store_true", help="Skip chunking step")
    parser.add_argument("--chunk-size", type=int, default=1000, help="Semantic chunk size")
    parser.add_argument("--chunk-overlap", type=int, default=200, help="Chunk overlap")
    parser.add_argument("--max-articles", type=int, default=None, help="Limit number of articles for semantic chunker")


    args = parser.parse_args()

    # Set TEST_MODE environment variable
    if args.test:
        os.environ["TEST_MODE"] = "1"
    else:
        os.environ["TEST_MODE"] = "0" # Explicitly set to "0" if not test
    global TEST_MODE # Update global TEST_MODE variable
    TEST_MODE = os.environ.get("TEST_MODE") == "1"

    os.environ["NUM_CONSUMER_THREADS"] = "1" # Or allow this to be an argument

    extracted_xml_path = EXTRACTED_PATH

    if args.download:
        if not download_wikipedia_dump(args.url, WIKI_ZIP_PATH):
            sys.exit(1)
        if not unzip_wikipedia_dump(WIKI_ZIP_PATH):
            sys.exit(1)

    elif args.compressed:
        if args.compressed != WIKI_ZIP_PATH:
            print(f"📁 Using compressed file: {args.compressed}")
        if not unzip_wikipedia_dump(args.compressed):
            sys.exit(1)

    elif args.extracted:
        extracted_xml_path = args.extracted
        print(f"📁 Using extracted XML: {extracted_xml_path}")
        if not extracted_xml_path.exists():
            print(f"❌ File not found: {extracted_xml_path}")
            sys.exit(1)
    # Extraction phase
    if not args.skip_extraction:
        if args.semantic:
            # Pass chunking arguments to semantic extractor
            if not run_semantic_extractor(
                extracted_xml_path,
                args.chunk_size,
                args.chunk_overlap,
                max_articles=args.max_articles  # <<< ADD max_articles here too!
            ):
                print("❌ Semantic extraction failed!")
                sys.exit(1)
        else:
            if args.extracted is None:
                if not run_basic_extractor():
                    print("❌ Basic extraction failed!")
                    sys.exit(1)

        # Embedding phase
        try:
            run_embedding_pipeline()
        except Exception as e:
            print(f"❌ Embedding failed: {e}")
            sys.exit(1)

        print("✅ Pipeline completed successfully!")

if __name__ == "__main__":
    main()

