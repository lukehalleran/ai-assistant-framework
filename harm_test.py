def evaluate_harm(text):
    return {"total_score": 0.1, "triggers": [], "action": "allow"}

def redact_harmful_phrases(text, triggers):
    return text
